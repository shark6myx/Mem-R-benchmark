{
  "sample_idx": 2,
  "total_questions": 193,
  "individual_results": [
    {
      "sample_id": 0,
      "question": "Who did Maria have dinner with on May 3, 2023?",
      "prediction": "family",
      "reference": "her mother",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9011951088905334,
        "bert_recall": 0.8508667349815369,
        "bert_f1": 0.875308096408844,
        "meteor": 0.0,
        "sbert_similarity": 0.5318413972854614
      }
    },
    {
      "sample_id": 0,
      "question": "When did Maria donate her car?",
      "prediction": "yesterday, 22 December, 2022",
      "reference": "21 December 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.08164965809277261,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.048549177170732344,
        "bert_precision": 0.9038418531417847,
        "bert_recall": 0.9547306299209595,
        "bert_f1": 0.9285895824432373,
        "meteor": 0.16129032258064518,
        "sbert_similarity": 0.8984649181365967
      }
    },
    {
      "sample_id": 0,
      "question": "What martial arts has John done?",
      "prediction": "kickboxing",
      "reference": "Kickboxing, Taekwondo",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.9487547278404236,
        "bert_recall": 0.8351958990097046,
        "bert_f1": 0.8883610367774963,
        "meteor": 0.0,
        "sbert_similarity": 0.7872939109802246
      }
    },
    {
      "sample_id": 0,
      "question": "What type of volunteering have John and Maria both done?",
      "prediction": "volunteering at the shelter",
      "reference": "Volunteering at a homeless shelter",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666665,
        "rouge1_f": 0.6666666666666665,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.6666666666666665,
        "bleu1": 0.5841005873035536,
        "bleu2": 0.3894003915357024,
        "bleu3": 0.1834031049364524,
        "bleu4": 0.14643803168582153,
        "bert_precision": 0.9473013877868652,
        "bert_recall": 0.8907865881919861,
        "bert_f1": 0.9181751608848572,
        "meteor": 0.5215419501133786,
        "sbert_similarity": 0.9230014085769653
      }
    },
    {
      "sample_id": 0,
      "question": "When did John join the online support group?",
      "prediction": "on 3 August, 2023",
      "reference": "The week before 1 January 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.1637461506155964,
        "bleu2": 0.05789300674674099,
        "bleu3": 0.046382183633183686,
        "bleu4": 0.0439891724758422,
        "bert_precision": 0.901520311832428,
        "bert_recall": 0.8712654113769531,
        "bert_f1": 0.886134684085846,
        "meteor": 0.08620689655172413,
        "sbert_similarity": 0.7138597965240479
      }
    },
    {
      "sample_id": 0,
      "question": "When did Maria go to the beach?",
      "prediction": "last month",
      "reference": "December 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8799847364425659,
        "bert_recall": 0.8896273374557495,
        "bert_f1": 0.8847797513008118,
        "meteor": 0.0,
        "sbert_similarity": 0.44718751311302185
      }
    },
    {
      "sample_id": 0,
      "question": "Where has Maria made friends?",
      "prediction": "Maria has made friends during her solo trip in Spain.",
      "reference": "homeless shelter, gym, church",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8438408374786377,
        "bert_recall": 0.813329815864563,
        "bert_f1": 0.8283044099807739,
        "meteor": 0.0,
        "sbert_similarity": 0.11273746192455292
      }
    },
    {
      "sample_id": 0,
      "question": "What items des John mention having as a child?",
      "prediction": "a little doll",
      "reference": "A doll, a film camera",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.24525296078096154,
        "bleu2": 0.06716528945733123,
        "bleu3": 0.05600870148614917,
        "bleu4": 0.04970787578611783,
        "bert_precision": 0.9014391899108887,
        "bert_recall": 0.8742614984512329,
        "bert_f1": 0.8876423239707947,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.6049355268478394
      }
    },
    {
      "sample_id": 0,
      "question": "What might John's financial status be?",
      "prediction": "John expresses gratitude for emotional support that helps him stay focused on making a positive impact.",
      "reference": "Middle-class or wealthy",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8516579866409302,
        "bert_recall": 0.8352117538452148,
        "bert_f1": 0.8433546423912048,
        "meteor": 0.0,
        "sbert_similarity": 0.07125139236450195
      }
    },
    {
      "sample_id": 0,
      "question": "Who gave Maria's family money when she was younger and her family was going through tough times?",
      "prediction": "Speaker Johnsays : I'm really thankful for their love. Family time means a lot to me.",
      "reference": "Her aunt",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8306225538253784,
        "bert_recall": 0.8503621816635132,
        "bert_f1": 0.8403764367103577,
        "meteor": 0.0,
        "sbert_similarity": 0.2033003866672516
      }
    },
    {
      "sample_id": 0,
      "question": "When did Maria meet Jean?",
      "prediction": "4 May, 2023",
      "reference": "February 24, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.5,
        "bleu2": 0.408248290463863,
        "bleu3": 0.20600140803015504,
        "bleu4": 0.16990442448471224,
        "bert_precision": 0.9424117207527161,
        "bert_recall": 0.9406934976577759,
        "bert_f1": 0.9415518045425415,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.7989124655723572
      }
    },
    {
      "sample_id": 0,
      "question": "What people has Maria met and helped while volunteering?",
      "prediction": "the people at the shelter",
      "reference": "David, Jean, Cindy, Laura",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.867999792098999,
        "bert_recall": 0.8167582750320435,
        "bert_f1": 0.8415997624397278,
        "meteor": 0.0,
        "sbert_similarity": 0.28112900257110596
      }
    },
    {
      "sample_id": 0,
      "question": "What test has John taken multiple times?",
      "prediction": "military aptitude test",
      "reference": "The military aptitude test",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.8571428571428571,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8571428571428571,
        "bleu1": 0.7165313105737893,
        "bleu2": 0.7165313105737893,
        "bleu3": 0.7165313105737893,
        "bleu4": 0.40293516672844226,
        "bert_precision": 0.9845775961875916,
        "bert_recall": 0.951336145401001,
        "bert_f1": 0.9676714539527893,
        "meteor": 0.754985754985755,
        "sbert_similarity": 0.9632324576377869
      }
    },
    {
      "sample_id": 0,
      "question": "When did Maria's grandmother pass away?",
      "prediction": "No mention of grandmother's passing.",
      "reference": "The week before 6 March 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8518428802490234,
        "bert_recall": 0.7952502965927124,
        "bert_f1": 0.8225743770599365,
        "meteor": 0.0,
        "sbert_similarity": 0.03745478391647339
      }
    },
    {
      "sample_id": 0,
      "question": "Would John be considered a patriotic person?",
      "prediction": "John emphasizes the importance of community support and engagement in advocating for those in need, including military veterans, highlighting the value of support and respect for the military.",
      "reference": "Yes",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7968723773956299,
        "bert_recall": 0.8476654887199402,
        "bert_f1": 0.8214845061302185,
        "meteor": 0.0,
        "sbert_similarity": 0.0834958404302597
      }
    },
    {
      "sample_id": 0,
      "question": "What writing classes has Maria taken?",
      "prediction": "creative writing class, poetry class",
      "reference": "Poetry, creative writing",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.7499999999999999,
        "rouge2_f": 0.3333333333333333,
        "rougeL_f": 0.5,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.36514837167011077,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.10266900960803409,
        "bert_precision": 0.8828020691871643,
        "bert_recall": 0.8929826617240906,
        "bert_f1": 0.8878631591796875,
        "meteor": 0.5859375,
        "sbert_similarity": 0.751309871673584
      }
    },
    {
      "sample_id": 0,
      "question": "When did John get his degree?",
      "prediction": "last week, 2 April 2023",
      "reference": "The week before 2 April 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7272727272727272,
        "rouge1_f": 0.7272727272727272,
        "rouge2_f": 0.4444444444444445,
        "rougeL_f": 0.7272727272727272,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5163977794943222,
        "bleu3": 0.4091569047284194,
        "bleu4": 0.21711852081087685,
        "bert_precision": 0.952927827835083,
        "bert_recall": 0.9459627866744995,
        "bert_f1": 0.9494324922561646,
        "meteor": 0.4990583804143125,
        "sbert_similarity": 0.8759219646453857
      }
    },
    {
      "sample_id": 0,
      "question": "What might John's degree be in?",
      "prediction": "policymaking",
      "reference": "Political science, Public administration, Public affairs",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8111631274223328,
        "bert_recall": 0.7705330848693848,
        "bert_f1": 0.7903262376785278,
        "meteor": 0.0,
        "sbert_similarity": 0.45792651176452637
      }
    },
    {
      "sample_id": 0,
      "question": "Who did John go to yoga with?",
      "prediction": "a colleague",
      "reference": "Rob",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8432379364967346,
        "bert_recall": 0.9023208618164062,
        "bert_f1": 0.8717795014381409,
        "meteor": 0.0,
        "sbert_similarity": 0.3000980615615845
      }
    },
    {
      "sample_id": 0,
      "question": "What damages have happened to John's car?",
      "prediction": "my car broke down last Fri on my way to work; a photo of a car with a broken windshield and a broken windshield",
      "reference": "Broken windshield, Car broke down",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.45454545454545453,
        "rouge1_f": 0.3448275862068966,
        "rouge2_f": 0.22222222222222218,
        "rougeL_f": 0.20689655172413793,
        "bleu1": 0.2,
        "bleu2": 0.158113883008419,
        "bleu3": 0.10518408103232378,
        "bleu4": 0.04714627368490469,
        "bert_precision": 0.8607151508331299,
        "bert_recall": 0.9096859693527222,
        "bert_f1": 0.8845232725143433,
        "meteor": 0.4574275362318841,
        "sbert_similarity": 0.7523655295372009
      }
    },
    {
      "sample_id": 0,
      "question": "When did John take a road trip to the Pacific Northwest?",
      "prediction": "10 April, 2023",
      "reference": "2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7679119110107422,
        "bert_recall": 0.8630644679069519,
        "bert_f1": 0.8127124905586243,
        "meteor": 0.0,
        "sbert_similarity": 0.6445618271827698
      }
    },
    {
      "sample_id": 0,
      "question": "What areas of the U.S. has John been to or is planning to go to?",
      "prediction": "East Coast, Pacific Northwest, Oregon",
      "reference": "Pacific northwest, east coast",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.888888888888889,
        "rouge1_f": 0.888888888888889,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.4444444444444445,
        "bleu1": 0.7142857142857143,
        "bleu2": 0.5976143046671969,
        "bleu3": 0.4185793008243382,
        "bleu4": 0.20556680845025987,
        "bert_precision": 0.8513649702072144,
        "bert_recall": 0.8699901700019836,
        "bert_f1": 0.8605767488479614,
        "meteor": 0.6233062330623306,
        "sbert_similarity": 0.9247276782989502
      }
    },
    {
      "sample_id": 0,
      "question": "When did John go to a convention with colleagues?",
      "prediction": "last month",
      "reference": "March 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8722803592681885,
        "bert_recall": 0.8398616313934326,
        "bert_f1": 0.8557640314102173,
        "meteor": 0.0,
        "sbert_similarity": 0.4508730173110962
      }
    },
    {
      "sample_id": 0,
      "question": "What desserts has Maria made?",
      "prediction": "The context does not mention any desserts that Maria has made.",
      "reference": "Banana split sundae, Peach cobbler",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8408015966415405,
        "bert_recall": 0.8105643391609192,
        "bert_f1": 0.8254061341285706,
        "meteor": 0.0,
        "sbert_similarity": 0.18243318796157837
      }
    },
    {
      "sample_id": 0,
      "question": "When did John start boot camp with his family?",
      "prediction": "last month (April 2023)",
      "reference": "April.2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.854432225227356,
        "bert_recall": 0.9115057587623596,
        "bert_f1": 0.8820466995239258,
        "meteor": 0.0,
        "sbert_similarity": 0.7866660356521606
      }
    },
    {
      "sample_id": 0,
      "question": "What European countries has Maria been to?",
      "prediction": "Spain",
      "reference": "Spain, England",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.8953713774681091,
        "bert_recall": 0.814091682434082,
        "bert_f1": 0.8527991771697998,
        "meteor": 0.0,
        "sbert_similarity": 0.8916140794754028
      }
    },
    {
      "sample_id": 0,
      "question": "What has Maria done to feel closer to her faith?",
      "prediction": "I bought a cross necklace to feel closer to my faith.",
      "reference": "Join a local church, buy a cross necklace",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3529411764705882,
        "rouge1_f": 0.3157894736842105,
        "rouge2_f": 0.23529411764705882,
        "rougeL_f": 0.3157894736842105,
        "bleu1": 0.25,
        "bleu2": 0.21320071635561041,
        "bleu3": 0.16865550266337087,
        "bleu4": 0.08430118946484098,
        "bert_precision": 0.8999626636505127,
        "bert_recall": 0.9152222871780396,
        "bert_f1": 0.9075283408164978,
        "meteor": 0.4781626506024097,
        "sbert_similarity": 0.6439568996429443
      }
    },
    {
      "sample_id": 0,
      "question": "When did John have a party with veterans?",
      "prediction": "20 May, 2023",
      "reference": "The Friday before 20 May 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.45489799478447507,
        "bleu2": 0.30326532985631666,
        "bleu3": 0.14283448174223617,
        "bleu4": 0.11404605374835301,
        "bert_precision": 0.9347727298736572,
        "bert_recall": 0.8873125314712524,
        "bert_f1": 0.9104245901107788,
        "meteor": 0.17543859649122806,
        "sbert_similarity": 0.8070908784866333
      }
    },
    {
      "sample_id": 0,
      "question": "What causes does John feel passionate about supporting?",
      "prediction": "making education better in our area",
      "reference": "Veterans, schools, infrastructure",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8305878639221191,
        "bert_recall": 0.831005334854126,
        "bert_f1": 0.8307965397834778,
        "meteor": 0.0,
        "sbert_similarity": 0.45746946334838867
      }
    },
    {
      "sample_id": 0,
      "question": "What events is Maria planning for the homeless shelter funraiser?",
      "prediction": "getting ready for a fundraiser next week",
      "reference": "Chili cook-off, ring-toss tournament",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8478076457977295,
        "bert_recall": 0.8500303030014038,
        "bert_f1": 0.8489174842834473,
        "meteor": 0.0,
        "sbert_similarity": 0.1472804695367813
      }
    },
    {
      "sample_id": 0,
      "question": "What shelters does Maria volunteer at?",
      "prediction": "the homeless shelter",
      "reference": "The homeless shelter, the dog shelter",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.2635971381157267,
        "bleu2": 0.2635971381157267,
        "bleu3": 0.2635971381157267,
        "bleu4": 0.14823156396438122,
        "bert_precision": 0.9500139951705933,
        "bert_recall": 0.8938761949539185,
        "bert_f1": 0.9210905432701111,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.8757314085960388
      }
    },
    {
      "sample_id": 0,
      "question": "When did John get his dog Max?",
      "prediction": "10 years before 3 June 2023",
      "reference": "In 2013",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8110976219177246,
        "bert_recall": 0.8838305473327637,
        "bert_f1": 0.8459035158157349,
        "meteor": 0.0,
        "sbert_similarity": 0.3770773708820343
      }
    },
    {
      "sample_id": 0,
      "question": "What outdoor activities has John done with his colleagues?",
      "prediction": "I went on a mountaineering trip last week with some workmates.",
      "reference": "Hiking, mountaineering",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615385,
        "rouge1_f": 0.15384615384615385,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615385,
        "bleu1": 0.08333333333333333,
        "bleu2": 0.027524094128159027,
        "bleu3": 0.020427282860665447,
        "bleu4": 0.017033186037639283,
        "bert_precision": 0.857921302318573,
        "bert_recall": 0.882959246635437,
        "bert_f1": 0.8702602386474609,
        "meteor": 0.1724137931034483,
        "sbert_similarity": 0.572149395942688
      }
    },
    {
      "sample_id": 0,
      "question": "What types of yoga has Maria practiced?",
      "prediction": "aerial yoga",
      "reference": "Aerial, kundalini",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.13562437855552414,
        "bleu3": 0.10556327625176448,
        "bleu4": 0.09069748827745895,
        "bert_precision": 0.8708494901657104,
        "bert_recall": 0.8144502639770508,
        "bert_f1": 0.8417062163352966,
        "meteor": 0.0,
        "sbert_similarity": 0.4823237955570221
      }
    },
    {
      "sample_id": 0,
      "question": "When did Maria join a gym?",
      "prediction": "last week, on June 16, 2023",
      "reference": "The week before 16 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.5,
        "bleu2": 0.08451542547285168,
        "bleu3": 0.0506976507883329,
        "bleu4": 0.039281465090051315,
        "bert_precision": 0.9286903142929077,
        "bert_recall": 0.9291975498199463,
        "bert_f1": 0.9289438128471375,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8470555543899536
      }
    },
    {
      "sample_id": 0,
      "question": "What states has Maria vacationed at?",
      "prediction": "Spain, Florida",
      "reference": "Oregon, Florida",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.24028114141347542,
        "bert_precision": 0.9985476136207581,
        "bert_recall": 0.9985476136207581,
        "bert_f1": 0.9985476136207581,
        "meteor": 0.25,
        "sbert_similarity": 0.6574999094009399
      }
    },
    {
      "sample_id": 0,
      "question": "What music events has John attended?",
      "prediction": "Last week, we had a blast at a live music event.",
      "reference": "Live music event, violin concert",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.37499999999999994,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.37499999999999994,
        "bleu1": 0.30769230769230765,
        "bleu2": 0.22645540682891918,
        "bleu3": 0.17007050142090102,
        "bleu4": 0.08263103182212594,
        "bert_precision": 0.8995411396026611,
        "bert_recall": 0.9151113033294678,
        "bert_f1": 0.9072594046592712,
        "meteor": 0.3348214285714286,
        "sbert_similarity": 0.6638715267181396
      }
    },
    {
      "sample_id": 0,
      "question": "What events for veterans has John participated in?",
      "prediction": "I just participated in a marching event for veterans' rights.",
      "reference": "Petition, march, party, visiting veterans hospital, 5K charity run",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.2105263157894737,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2105263157894737,
        "bleu1": 0.07667036788577694,
        "bleu2": 0.025323389070344468,
        "bleu3": 0.01879400750200855,
        "bleu4": 0.015671287677272196,
        "bert_precision": 0.8723756074905396,
        "bert_recall": 0.8472045660018921,
        "bert_f1": 0.8596058487892151,
        "meteor": 0.0,
        "sbert_similarity": 0.5573165416717529
      }
    },
    {
      "sample_id": 0,
      "question": "When did Maria get in a car accident?",
      "prediction": "yesterday, 3 July 2023",
      "reference": "July 2, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.6,
        "bleu2": 0.12247448713915889,
        "bleu3": 0.081406690893008,
        "bleu4": 0.07071067811865477,
        "bert_precision": 0.9221535921096802,
        "bert_recall": 0.9439716339111328,
        "bert_f1": 0.9329351186752319,
        "meteor": 0.32258064516129037,
        "sbert_similarity": 0.8456143140792847
      }
    },
    {
      "sample_id": 0,
      "question": "Around which US holiday did Maria get into a car accident?",
      "prediction": "July 4th",
      "reference": "Independence Day",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8513538837432861,
        "bert_recall": 0.8828967213630676,
        "bert_f1": 0.8668384552001953,
        "meteor": 0.0,
        "sbert_similarity": 0.5867877006530762
      }
    },
    {
      "sample_id": 0,
      "question": "What are the names of John's children?",
      "prediction": "The names of John's children are not mentioned in the provided context.",
      "reference": "Kyle, Sara",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8335782289505005,
        "bert_recall": 0.7992720007896423,
        "bert_f1": 0.816064715385437,
        "meteor": 0.0,
        "sbert_similarity": 0.10237100720405579
      }
    },
    {
      "sample_id": 0,
      "question": "Does John live close to a beach or the mountains?",
      "prediction": "John went on a mountaineering trip last week with some workmates.",
      "reference": "beach",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7996740341186523,
        "bert_recall": 0.8529049158096313,
        "bert_f1": 0.8254321813583374,
        "meteor": 0.0,
        "sbert_similarity": 0.15873250365257263
      }
    },
    {
      "sample_id": 0,
      "question": "What area was hit by a flood?",
      "prediction": "My old area was hit by a nasty flood last week.",
      "reference": "West County",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8395578861236572,
        "bert_recall": 0.8110110759735107,
        "bert_f1": 0.8250375986099243,
        "meteor": 0.0,
        "sbert_similarity": 0.18738719820976257
      }
    },
    {
      "sample_id": 0,
      "question": "When was John's old area hit with a flood?",
      "prediction": "last week, 1 July 2023",
      "reference": "The week before 7 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.5454545454545454,
        "rouge2_f": 0.22222222222222224,
        "rougeL_f": 0.5454545454545454,
        "bleu1": 0.5,
        "bleu2": 0.316227766016838,
        "bleu3": 0.1384586850774637,
        "bleu4": 0.09554427922043669,
        "bert_precision": 0.9476995468139648,
        "bert_recall": 0.9414031505584717,
        "bert_f1": 0.9445408582687378,
        "meteor": 0.3177966101694915,
        "sbert_similarity": 0.8381909728050232
      }
    },
    {
      "sample_id": 0,
      "question": "What activities has Maria done with her church friends?",
      "prediction": "We chilled under the trees, played games, and ate yummy food.",
      "reference": "Hiking, picnic, volunteer work",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.033149677206589796,
        "bleu3": 0.021746357799665227,
        "bleu4": 0.0169861974906263,
        "bert_precision": 0.8574515581130981,
        "bert_recall": 0.840143084526062,
        "bert_f1": 0.8487090468406677,
        "meteor": 0.10638297872340427,
        "sbert_similarity": 0.4074179530143738
      }
    },
    {
      "sample_id": 0,
      "question": "Would John be open to moving to another country?",
      "prediction": "The context does not provide any information regarding John's openness to moving to another country.",
      "reference": "No, he has goals specifically in the U.S. like joining the military and running for office.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.06666666666666667,
        "rouge1_f": 0.12121212121212122,
        "rouge2_f": 0.0,
        "rougeL_f": 0.12121212121212122,
        "bleu1": 0.11092625221822057,
        "bleu2": 0.025567237845169212,
        "bleu3": 0.016683044588115405,
        "bleu4": 0.012897742882522783,
        "bert_precision": 0.8502644300460815,
        "bert_recall": 0.8796885013580322,
        "bert_f1": 0.8647262454032898,
        "meteor": 0.06289308176100629,
        "sbert_similarity": 0.29622161388397217
      }
    },
    {
      "sample_id": 0,
      "question": "When did Maria go hiking with her church friends?",
      "prediction": "last weekend on 22 July, 2023",
      "reference": "The weekend before 22 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.4000000000000001,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5714285714285714,
        "bleu2": 0.3086066999241838,
        "bleu3": 0.12657485247324374,
        "bleu4": 0.08307018474412793,
        "bert_precision": 0.9416148662567139,
        "bert_recall": 0.9505760073661804,
        "bert_f1": 0.9460741877555847,
        "meteor": 0.25,
        "sbert_similarity": 0.8868173360824585
      }
    },
    {
      "sample_id": 0,
      "question": "What exercises has John done?",
      "prediction": "kickboxing and yoga",
      "reference": "Weight training, Circuit training, Kickboxing, yoga",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0.09022352215774179,
        "bleu2": 0.02470872915168118,
        "bleu3": 0.02060444980346269,
        "bleu4": 0.018286505566016496,
        "bert_precision": 0.8933291435241699,
        "bert_recall": 0.8556358814239502,
        "bert_f1": 0.8740763664245605,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.8076305985450745
      }
    },
    {
      "sample_id": 0,
      "question": "When did John have his first firefighter call-out?",
      "prediction": "Last Sunday, 31 July 2023.",
      "reference": "The sunday before 3` July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.5454545454545454,
        "rouge2_f": 0.22222222222222224,
        "rougeL_f": 0.5454545454545454,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.26726124191242434,
        "bleu3": 0.11511121735118796,
        "bleu4": 0.07730551756939454,
        "bert_precision": 0.9220337271690369,
        "bert_recall": 0.8846083879470825,
        "bert_f1": 0.9029334783554077,
        "meteor": 0.0847457627118644,
        "sbert_similarity": 0.8448472023010254
      }
    },
    {
      "sample_id": 0,
      "question": "What food item did Maria drop off at the homeless shelter?",
      "prediction": "[Image: a photography of a group of people standing around a table with boxes of tomatoes]",
      "reference": "Cakes",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7947925329208374,
        "bert_recall": 0.7859976291656494,
        "bert_f1": 0.7903706431388855,
        "meteor": 0.0,
        "sbert_similarity": 0.28341177105903625
      }
    },
    {
      "sample_id": 0,
      "question": "What attributes describe John?",
      "prediction": "passion, dedication, resilience, resourcefulness, hard work, kindness, commitment, creativity, support, inspiration",
      "reference": "Selfless, family-oriented, passionate, rational",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.12500000000000003,
        "rouge2_f": 0.0,
        "rougeL_f": 0.12500000000000003,
        "bleu1": 0.15,
        "bleu2": 0.02809757434745082,
        "bleu3": 0.017056169390452146,
        "bleu4": 0.012673718536830811,
        "bert_precision": 0.8585939407348633,
        "bert_recall": 0.8642700910568237,
        "bert_f1": 0.8614227175712585,
        "meteor": 0.0,
        "sbert_similarity": 0.5248630046844482
      }
    },
    {
      "sample_id": 0,
      "question": "When did Maria start volunteering at the homeless shelter?",
      "prediction": "about a year ago",
      "reference": "Around August 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7851760387420654,
        "bert_recall": 0.8263900279998779,
        "bert_f1": 0.8052560091018677,
        "meteor": 0.16129032258064518,
        "sbert_similarity": 0.4597036838531494
      }
    },
    {
      "sample_id": 0,
      "question": "Who have written notes of gratitude to Maria?",
      "prediction": "Speaker John",
      "reference": "Cindy, Laura",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8731136918067932,
        "bert_recall": 0.834547221660614,
        "bert_f1": 0.8533949851989746,
        "meteor": 0.0,
        "sbert_similarity": 0.17625880241394043
      }
    },
    {
      "sample_id": 0,
      "question": "When did John help renovate his hometown community center?",
      "prediction": "last year",
      "reference": "2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8694188594818115,
        "bert_recall": 0.9111441373825073,
        "bert_f1": 0.8897925615310669,
        "meteor": 0.0,
        "sbert_similarity": 0.3672372102737427
      }
    },
    {
      "sample_id": 0,
      "question": "When did Maria take up community work with her church friends?",
      "prediction": "yesterday, 5 August 2023",
      "reference": "August 4, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.6,
        "bleu2": 0.12247448713915889,
        "bleu3": 0.081406690893008,
        "bleu4": 0.07071067811865477,
        "bert_precision": 0.9186701774597168,
        "bert_recall": 0.9351404905319214,
        "bert_f1": 0.9268321394920349,
        "meteor": 0.32258064516129037,
        "sbert_similarity": 0.8495193719863892
      }
    },
    {
      "sample_id": 0,
      "question": "When did Maria receive a medal from the homeless shelter?",
      "prediction": "9 August, 2023",
      "reference": "The week before 9 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.45489799478447507,
        "bleu2": 0.30326532985631666,
        "bleu3": 0.14283448174223617,
        "bleu4": 0.11404605374835301,
        "bert_precision": 0.9309033155441284,
        "bert_recall": 0.886680006980896,
        "bert_f1": 0.9082536697387695,
        "meteor": 0.17543859649122806,
        "sbert_similarity": 0.8698515892028809
      }
    },
    {
      "sample_id": 0,
      "question": "When did John participate in a 5K charity run?",
      "prediction": "Last weekend, on 9 August, 2023.",
      "reference": "first weekend of August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.5454545454545454,
        "rouge2_f": 0.22222222222222224,
        "rougeL_f": 0.5454545454545454,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.06454972243679029,
        "bleu3": 0.04033183084278514,
        "bleu4": 0.03155984539112946,
        "bert_precision": 0.9197759032249451,
        "bert_recall": 0.9401488304138184,
        "bert_f1": 0.9298507571220398,
        "meteor": 0.0,
        "sbert_similarity": 0.7727963924407959
      }
    },
    {
      "sample_id": 0,
      "question": "What causes has John done events for?",
      "prediction": "events for various causes",
      "reference": "Toy drive, Community food drive, veterans, domestic violence",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8268226385116577,
        "bert_recall": 0.8207712173461914,
        "bert_f1": 0.8237857818603516,
        "meteor": 0.0,
        "sbert_similarity": 0.25091060996055603
      }
    },
    {
      "sample_id": 0,
      "question": "When did Maria get Coco?",
      "prediction": "two weeks ago",
      "reference": "Two weeks before 11 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444444,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0.24525296078096154,
        "bleu2": 0.21239529438966132,
        "bleu3": 0.11974448045963429,
        "bleu4": 0.08839449202722564,
        "bert_precision": 0.828338623046875,
        "bert_recall": 0.8085601925849915,
        "bert_f1": 0.8183299899101257,
        "meteor": 0.3289473684210526,
        "sbert_similarity": 0.5115441083908081
      }
    },
    {
      "sample_id": 0,
      "question": "When did John go on a camping trip with Max?",
      "prediction": "last summer",
      "reference": "The summer of 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.18393972058572117,
        "bleu2": 0.082260343798398,
        "bleu3": 0.06402736358640969,
        "bleu4": 0.05501080739920602,
        "bert_precision": 0.9266855716705322,
        "bert_recall": 0.8821314573287964,
        "bert_f1": 0.9038597941398621,
        "meteor": 0.13157894736842105,
        "sbert_similarity": 0.5159845352172852
      }
    },
    {
      "sample_id": 0,
      "question": "What are Maria's dogs' names?",
      "prediction": "Coco and Shadow",
      "reference": "Coco, Shadow",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.0,
        "rougeL_f": 0.8,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.18257418583505536,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.13512001548070346,
        "bert_precision": 0.9198997616767883,
        "bert_recall": 0.9198997616767883,
        "bert_f1": 0.9198997616767883,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.9564536809921265
      }
    },
    {
      "sample_id": 0,
      "question": "When did Maria adopt Shadow?",
      "prediction": "3:14 pm on 13 August, 2023",
      "reference": "The week before 13 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.4615384615384615,
        "rouge2_f": 0.3636363636363636,
        "rougeL_f": 0.4615384615384615,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.26726124191242434,
        "bleu3": 0.11511121735118796,
        "bleu4": 0.07730551756939454,
        "bert_precision": 0.8793649673461914,
        "bert_recall": 0.8848503828048706,
        "bert_f1": 0.8820991516113281,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.7386160492897034
      }
    },
    {
      "sample_id": 0,
      "question": "How many dogs has Maria adopted from the dog shelter she volunteers at?",
      "prediction": "I just adopted this cute pup from a shelter last week.",
      "reference": "two",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8041139841079712,
        "bert_recall": 0.8267748951911926,
        "bert_f1": 0.8152869939804077,
        "meteor": 0.0,
        "sbert_similarity": 0.05986513942480087
      }
    },
    {
      "sample_id": 0,
      "question": "How many weeks passed between Maria adopting Coco and Shadow?",
      "prediction": "1 week",
      "reference": "two weeks",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9606261253356934,
        "bert_recall": 0.9606261253356934,
        "bert_f1": 0.9606261253356934,
        "meteor": 0.25,
        "sbert_similarity": 0.8272432088851929
      }
    },
    {
      "sample_id": 0,
      "question": "What job might Maria pursue in the future?",
      "prediction": "future projects",
      "reference": "Shelter coordinator, Counselor",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8813700675964355,
        "bert_recall": 0.8311275243759155,
        "bert_f1": 0.8555117845535278,
        "meteor": 0.0,
        "sbert_similarity": 0.1781536489725113
      }
    },
    {
      "sample_id": 0,
      "question": "What is John's main focus in local politics?",
      "prediction": "helping the community and making it a better place",
      "reference": "Improving education and infrastructure",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615383,
        "rouge1_f": 0.15384615384615383,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615383,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.8650351166725159,
        "bert_recall": 0.8598058819770813,
        "bert_f1": 0.8624125123023987,
        "meteor": 0.11111111111111112,
        "sbert_similarity": 0.4122829735279083
      }
    },
    {
      "sample_id": 0,
      "question": "What sparked John's interest in improving education and infrastructure in the community?",
      "prediction": "Growing up, I saw how lack of education and crumbling infrastructure affected my neighborhood.",
      "reference": "Seeing how lack of education and crumbling infrastructure affected his neighborhood while growing up.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7857142857142857,
        "rouge1_f": 0.7857142857142857,
        "rouge2_f": 0.6153846153846154,
        "rougeL_f": 0.6428571428571429,
        "bleu1": 0.75,
        "bleu2": 0.6324555320336759,
        "bleu3": 0.5587884978647216,
        "bleu4": 0.5067309892897293,
        "bert_precision": 0.9496943354606628,
        "bert_recall": 0.951740026473999,
        "bert_f1": 0.9507160782814026,
        "meteor": 0.7114285714285714,
        "sbert_similarity": 0.8214857578277588
      }
    },
    {
      "sample_id": 0,
      "question": "How did the extra funding help the school shown in the photo shared by John?",
      "prediction": "I saw how lack of education and crumbling infrastructure affected my neighborhood.",
      "reference": "Enabled needed repairs and renovations, making the learning environment safer and more modern for students.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.07692307692307691,
        "rouge1_f": 0.07407407407407407,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07407407407407407,
        "bleu1": 0.11309868932179762,
        "bleu2": 0.026322287438753814,
        "bleu3": 0.017310918975848796,
        "bleu4": 0.013583106526228583,
        "bert_precision": 0.882750391960144,
        "bert_recall": 0.8676819801330566,
        "bert_f1": 0.8751513361930847,
        "meteor": 0.06802721088435373,
        "sbert_similarity": 0.37740230560302734
      }
    },
    {
      "sample_id": 0,
      "question": "What type of workout class did Maria start doing in December 2023?",
      "prediction": "The context does not specify a workout class that Maria started in December 2023.",
      "reference": "aerial yoga",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.822317361831665,
        "bert_recall": 0.8495383262634277,
        "bert_f1": 0.8357062339782715,
        "meteor": 0.0,
        "sbert_similarity": 0.13089895248413086
      }
    },
    {
      "sample_id": 0,
      "question": "What did Maria donate to a homeless shelter in December 2023?",
      "prediction": "donated my old car to a homeless shelter",
      "reference": "old car",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.25,
        "bleu2": 0.18898223650461357,
        "bleu3": 0.08622792534209149,
        "bleu4": 0.05873949094699214,
        "bert_precision": 0.8271936178207397,
        "bert_recall": 0.8997499942779541,
        "bert_f1": 0.8619475960731506,
        "meteor": 0.7211538461538463,
        "sbert_similarity": 0.5465290546417236
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of meal did John and his family make together in the photo shared by John?",
      "prediction": "dinner",
      "reference": "pizza",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.999267578125,
        "bert_recall": 0.999267578125,
        "bert_f1": 0.999267578125,
        "meteor": 0.0,
        "sbert_similarity": 0.5743187665939331
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of online group did John join?",
      "prediction": "a service-focused online group",
      "reference": "service-focused online group",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.888888888888889,
        "rouge2_f": 0.8571428571428571,
        "rougeL_f": 0.888888888888889,
        "bleu1": 0.75,
        "bleu2": 0.7071067811865475,
        "bleu3": 0.63287829698514,
        "bleu4": 0.3976353643835253,
        "bert_precision": 0.9270813465118408,
        "bert_recall": 0.9604374170303345,
        "bert_f1": 0.9434646368026733,
        "meteor": 0.9498207885304659,
        "sbert_similarity": 0.9781621694564819
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of activities did John and his mates from the online group do as part of their service efforts?",
      "prediction": "providing the right assistance and resources",
      "reference": "gave out food and supplies at a homeless shelter, organized a toy drive for kids in need",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09090909090909091,
        "rouge1_f": 0.08695652173913045,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08695652173913045,
        "bleu1": 0.02255588053943545,
        "bleu2": 0.00781358622075126,
        "bleu3": 0.006099317273192301,
        "bleu4": 0.005525039802078984,
        "bert_precision": 0.8505014181137085,
        "bert_recall": 0.8328558802604675,
        "bert_f1": 0.8415862321853638,
        "meteor": 0.031446540880503145,
        "sbert_similarity": 0.34754687547683716
      }
    },
    {
      "sample_id": 0,
      "question": "Who inspired Maria to start volunteering?",
      "prediction": "My aunt believed in volunteering, and used to help my family out when we were struggling, so I'm inspired by her.",
      "reference": "Her aunt",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.16666666666666669,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08333333333333334,
        "bleu1": 0.07999999999999999,
        "bleu2": 0.01825741858350553,
        "bleu3": 0.01183531442162675,
        "bleu4": 0.009009113474307318,
        "bert_precision": 0.8288921117782593,
        "bert_recall": 0.9131455421447754,
        "bert_f1": 0.8689813613891602,
        "meteor": 0.12820512820512822,
        "sbert_similarity": 0.45756471157073975
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Maria sit with the little girl at the shelter event in February 2023?",
      "prediction": "to help them succeed",
      "reference": "The girl seemed sad and had no other family",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8612314462661743,
        "bert_recall": 0.8305601477622986,
        "bert_f1": 0.8456177711486816,
        "meteor": 0.0,
        "sbert_similarity": 0.07259020954370499
      }
    },
    {
      "sample_id": 0,
      "question": "What did Jean go through before meeting Maria?",
      "prediction": "she had been through a lot",
      "reference": "divorce, job loss, homelessness",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8342751264572144,
        "bert_recall": 0.8025776147842407,
        "bert_f1": 0.818119466304779,
        "meteor": 0.0,
        "sbert_similarity": 0.2745027542114258
      }
    },
    {
      "sample_id": 0,
      "question": "Why did John decide to run for office again?",
      "prediction": "I guess I can't let go of my dream to make a difference in my community.",
      "reference": "saw the impact he could make in the community through politics",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.21428571428571427,
        "rouge2_f": 0.0,
        "rougeL_f": 0.21428571428571427,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.03131121455425748,
        "bleu3": 0.019045937245652737,
        "bleu4": 0.014216645907653848,
        "bert_precision": 0.8839614391326904,
        "bert_recall": 0.9057185053825378,
        "bert_f1": 0.8947077393531799,
        "meteor": 0.08695652173913045,
        "sbert_similarity": 0.3355077803134918
      }
    },
    {
      "sample_id": 0,
      "question": "What activity did John's colleague, Rob, invite him to?",
      "prediction": "a convention together",
      "reference": "beginner's yoga class",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8608928322792053,
        "bert_recall": 0.8486422300338745,
        "bert_f1": 0.8547235727310181,
        "meteor": 0.0,
        "sbert_similarity": 0.12793444097042084
      }
    },
    {
      "sample_id": 0,
      "question": "What is the name of John's one-year-old child?",
      "prediction": "Kyle",
      "reference": "Kyle",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "How often does John take his kids to the park?",
      "prediction": "a few times a week",
      "reference": "A few times a week",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9792031049728394,
        "bert_recall": 0.9792031049728394,
        "bert_f1": 0.9792031049728394,
        "meteor": 0.996,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What did Maria make for her home to remind her of a trip to England?",
      "prediction": "Last year I took a solo trip and took this pic in Spain. It reminded me that life is hard but there's still hope and beauty.",
      "reference": "painting of a castle on a hill",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.06666666666666667,
        "rouge1_f": 0.058823529411764705,
        "rouge2_f": 0.0,
        "rougeL_f": 0.058823529411764705,
        "bleu1": 0.034482758620689655,
        "bleu2": 0.011097419040461881,
        "bleu3": 0.008081583568827886,
        "bleu4": 0.006471824245088331,
        "bert_precision": 0.8478109240531921,
        "bert_recall": 0.8525952100753784,
        "bert_f1": 0.850196361541748,
        "meteor": 0.05617977528089888,
        "sbert_similarity": 0.15888187289237976
      }
    },
    {
      "sample_id": 0,
      "question": "Where did Maria get the idea for the castle shadow box in her home?",
      "prediction": "I got the idea from that trip to England a few years ago - I was mesmerized by the castles.",
      "reference": "England",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10526315789473684,
        "rouge1_f": 0.1,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1,
        "bleu1": 0.047619047619047616,
        "bleu2": 0.01543033499620919,
        "bleu3": 0.011280777824436245,
        "bleu4": 0.009134423666564471,
        "bert_precision": 0.7961043119430542,
        "bert_recall": 0.8316344022750854,
        "bert_f1": 0.8134815692901611,
        "meteor": 0.1724137931034483,
        "sbert_similarity": 0.3217815160751343
      }
    },
    {
      "sample_id": 0,
      "question": "What did John receive a certificate for?",
      "prediction": "graduation",
      "reference": "completion of a university degree",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8653659820556641,
        "bert_recall": 0.8052520751953125,
        "bert_f1": 0.834227442741394,
        "meteor": 0.0,
        "sbert_similarity": 0.5394787192344666
      }
    },
    {
      "sample_id": 0,
      "question": "What areas is John particularly interested in for policymaking?",
      "prediction": "Improving education and infrastructure is particularly interesting to me.",
      "reference": "education and infrastructure",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5,
        "bleu1": 0.3,
        "bleu2": 0.25819888974716104,
        "bleu3": 0.20600140803015504,
        "bleu4": 0.10445522730720382,
        "bert_precision": 0.8657068014144897,
        "bert_recall": 0.9134945869445801,
        "bert_f1": 0.8889588713645935,
        "meteor": 0.8179012345679013,
        "sbert_similarity": 0.6755564212799072
      }
    },
    {
      "sample_id": 0,
      "question": "What did Maria participate in last weekend before April 10, 2023?",
      "prediction": "volunteering at the homeless shelter",
      "reference": "a 5K charity run",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8793120384216309,
        "bert_recall": 0.8321521878242493,
        "bert_f1": 0.8550823926925659,
        "meteor": 0.0,
        "sbert_similarity": 0.37872517108917236
      }
    },
    {
      "sample_id": 0,
      "question": "What event did John volunteer at last weekend?",
      "prediction": "a community event to raise money for a good cause",
      "reference": "career fair at a local school",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.10000000000000002,
        "bleu2": 0.03333333333333335,
        "bleu3": 0.024950558772614845,
        "bleu4": 0.021105340631872645,
        "bert_precision": 0.8843598961830139,
        "bert_recall": 0.8885993957519531,
        "bert_f1": 0.8864745497703552,
        "meteor": 0.078125,
        "sbert_similarity": 0.28548958897590637
      }
    },
    {
      "sample_id": 0,
      "question": "What did John do that put a strain on his wallet?",
      "prediction": "my car broke down",
      "reference": "His car broke down",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.75,
        "rouge1_f": 0.75,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.75,
        "bleu1": 0.75,
        "bleu2": 0.7071067811865475,
        "bleu3": 0.63287829698514,
        "bleu4": 0.3976353643835253,
        "bert_precision": 0.9987967014312744,
        "bert_recall": 0.9987967014312744,
        "bert_f1": 0.9987967014312744,
        "meteor": 0.7361111111111112,
        "sbert_similarity": 0.8009454011917114
      }
    },
    {
      "sample_id": 0,
      "question": "Where did John explore on a road trip last year?",
      "prediction": "the coast up in the Pacific Northwest",
      "reference": "Pacific Northwest",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.4444444444444445,
        "rouge2_f": 0.2857142857142857,
        "rougeL_f": 0.4444444444444445,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.2182178902359924,
        "bleu3": 0.1006949130683619,
        "bleu4": 0.06985342056580097,
        "bert_precision": 0.8635777235031128,
        "bert_recall": 0.9146767258644104,
        "bert_f1": 0.8883930444717407,
        "meteor": 0.75,
        "sbert_similarity": 0.7400248646736145
      }
    },
    {
      "sample_id": 0,
      "question": "What topic has John been blogging about recently?",
      "prediction": "politics and the government",
      "reference": "politics and the government",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "Why did John start blogging about politics and policies?",
      "prediction": "It's been a really satisfying experience and I care about making a real impact. Hoping to raise awareness and start conversations to create positive change.",
      "reference": "raise awareness and start conversations to create positive change",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5806451612903226,
        "rouge1_f": 0.5142857142857142,
        "rouge2_f": 0.48484848484848486,
        "rougeL_f": 0.5142857142857142,
        "bleu1": 0.32142857142857145,
        "bleu2": 0.3086066999241838,
        "bleu3": 0.29850298588855284,
        "bleu4": 0.28008294138571455,
        "bert_precision": 0.8882431983947754,
        "bert_recall": 0.933336079120636,
        "bert_f1": 0.9102315306663513,
        "meteor": 0.7539799528301887,
        "sbert_similarity": 0.622825026512146
      }
    },
    {
      "sample_id": 0,
      "question": "What was the focus of John's recent research and writing on his blog?",
      "prediction": "improving education and infrastructure in our community",
      "reference": "education reform and infrastructure development",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.2,
        "rougeL_f": 0.5,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.26726124191242434,
        "bleu3": 0.11511121735118796,
        "bleu4": 0.07730551756939454,
        "bert_precision": 0.8862086534500122,
        "bert_recall": 0.8911867141723633,
        "bert_f1": 0.888690710067749,
        "meteor": 0.49145299145299143,
        "sbert_similarity": 0.8018261790275574
      }
    },
    {
      "sample_id": 0,
      "question": "What did John attend with his colleagues in March 2023?",
      "prediction": "collaboration, forces, achieve, great",
      "reference": "a tech-for-good convention",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7749840617179871,
        "bert_recall": 0.8372694253921509,
        "bert_f1": 0.8049236536026001,
        "meteor": 0.0,
        "sbert_similarity": 0.3570541739463806
      }
    },
    {
      "sample_id": 0,
      "question": "How often does John work out with his family?",
      "prediction": "I just started going to boot camps with my fam last month.",
      "reference": "Three times a week",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8336806893348694,
        "bert_recall": 0.8351141810417175,
        "bert_f1": 0.8343967795372009,
        "meteor": 0.0,
        "sbert_similarity": 0.1491509974002838
      }
    },
    {
      "sample_id": 0,
      "question": "How has John's fitness improved since starting boot camps with his family?",
      "prediction": "We've had lots more energy and noticed some gains in strength and endurance.",
      "reference": "More energy, gains in strength and endurance",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7368421052631579,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5263157894736842,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.4666666666666667,
        "bleu2": 0.408248290463863,
        "bleu3": 0.3412391057457694,
        "bleu4": 0.2829559628326351,
        "bert_precision": 0.9068131446838379,
        "bert_recall": 0.9343182444572449,
        "bert_f1": 0.9203602075576782,
        "meteor": 0.6368421052631579,
        "sbert_similarity": 0.7287561893463135
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of food did Maria have on her dinner spread iwth her mother?",
      "prediction": "a variety of food",
      "reference": "Salads, sandwiches, homemade desserts",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8448200225830078,
        "bert_recall": 0.8289343118667603,
        "bert_f1": 0.8368017673492432,
        "meteor": 0.0,
        "sbert_similarity": 0.5926780700683594
      }
    },
    {
      "sample_id": 0,
      "question": "What activity did Maria and her mom do together in May 2023?",
      "prediction": "Spending time with family",
      "reference": "Made dinner together",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8629189133644104,
        "bert_recall": 0.8682700991630554,
        "bert_f1": 0.8655862808227539,
        "meteor": 0.0,
        "sbert_similarity": 0.40288031101226807
      }
    },
    {
      "sample_id": 0,
      "question": "What did Maria do to feel closer to a community and her faith?",
      "prediction": "I joined a nearby church.",
      "reference": "joined a nearby church",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.888888888888889,
        "rouge1_f": 0.888888888888889,
        "rouge2_f": 0.8571428571428571,
        "rougeL_f": 0.888888888888889,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.6324555320336759,
        "bleu3": 0.5879493283318652,
        "bleu4": 0.5081327481546147,
        "bert_precision": 0.9394880533218384,
        "bert_recall": 0.9402550458908081,
        "bert_f1": 0.9398713707923889,
        "meteor": 0.7181571815718157,
        "sbert_similarity": 0.9113637208938599
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Maria join a nearby church recently?",
      "prediction": "I wanted to feel closer to a community and my faith.",
      "reference": "to feel closer to a community and her faith",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7777777777777777,
        "rouge1_f": 0.7999999999999999,
        "rouge2_f": 0.6666666666666665,
        "rougeL_f": 0.7999999999999999,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.6030226891555271,
        "bleu3": 0.5697447243773885,
        "bleu4": 0.5331675363405771,
        "bert_precision": 0.9505825638771057,
        "bert_recall": 0.9557753801345825,
        "bert_f1": 0.9531718492507935,
        "meteor": 0.7597604259094942,
        "sbert_similarity": 0.7266887426376343
      }
    },
    {
      "sample_id": 0,
      "question": "What did John host for the veterans in May 2023 as part of the project?",
      "prediction": "a petition",
      "reference": "a small party to share their stories",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.0410424993119494,
        "bleu2": 0.018354763685601443,
        "bleu3": 0.014286435890917338,
        "bleu4": 0.012274570264879268,
        "bert_precision": 0.8789434432983398,
        "bert_recall": 0.8405145406723022,
        "bert_f1": 0.8592996001243591,
        "meteor": 0.07692307692307691,
        "sbert_similarity": 0.11871220171451569
      }
    },
    {
      "sample_id": 0,
      "question": "What did John and the veterans do during the small party?",
      "prediction": "inviting some veterans to share their stories",
      "reference": "share stories and make connections",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.06900655593423542,
        "bleu3": 0.047098549390932394,
        "bleu4": 0.039281465090051315,
        "bert_precision": 0.8555154800415039,
        "bert_recall": 0.8782139420509338,
        "bert_f1": 0.8667161464691162,
        "meteor": 0.19230769230769232,
        "sbert_similarity": 0.5211460590362549
      }
    },
    {
      "sample_id": 0,
      "question": "What emotions did John feel during the small party with the veterans?",
      "prediction": "heartwarming, awesome, motivated",
      "reference": "heartwarming",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8698198795318604,
        "bert_recall": 0.961078405380249,
        "bert_f1": 0.9131748080253601,
        "meteor": 0.0,
        "sbert_similarity": 0.7415993809700012
      }
    },
    {
      "sample_id": 0,
      "question": "What event is Maria getting ready for at the shelter on May 25, 2023?",
      "prediction": "a fundraiser next week",
      "reference": "fundraiser",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.8085678815841675,
        "bert_recall": 0.8772625923156738,
        "bert_f1": 0.8415156006813049,
        "meteor": 0.3846153846153847,
        "sbert_similarity": 0.8453813195228577
      }
    },
    {
      "sample_id": 0,
      "question": "What does Maria need to spread the word about for the fundraiser for the volunteer shelter?",
      "prediction": "raise enough to cover basic needs for the homeless",
      "reference": "chili cook-off",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8411834239959717,
        "bert_recall": 0.8772318363189697,
        "bert_f1": 0.8588295578956604,
        "meteor": 0.0,
        "sbert_similarity": 0.034810520708560944
      }
    },
    {
      "sample_id": 0,
      "question": "What was the name of the pet that John had to say goodbye to on 3 June, 2023?",
      "prediction": "Max",
      "reference": "Max",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "How long was Max a part of John's family?",
      "prediction": "for 10 years",
      "reference": "10 years",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.24028114141347542,
        "bert_precision": 0.9292773604393005,
        "bert_recall": 0.9572228193283081,
        "bert_f1": 0.9430431127548218,
        "meteor": 0.8928571428571429,
        "sbert_similarity": 0.8128395080566406
      }
    },
    {
      "sample_id": 0,
      "question": "How does John plan to honor the memories of his beloved pet?",
      "prediction": "Let's make sure we make progress in the coming weeks to ensure that we honor the memories of our loved ones.",
      "reference": "By considering adopting a rescue dog",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8390200734138489,
        "bert_recall": 0.8398340344429016,
        "bert_f1": 0.8394268155097961,
        "meteor": 0.0,
        "sbert_similarity": 0.09707307070493698
      }
    },
    {
      "sample_id": 0,
      "question": "What important values does John want to teach his kids through adopting a rescue dog?",
      "prediction": "responsibility and compassion",
      "reference": "Responsibility and compassion",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9994602203369141,
        "bert_recall": 0.9994602203369141,
        "bert_f1": 0.9994602203369141,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What new activity did Maria start recently, as mentioned on 3 June, 2023?",
      "prediction": "The context does not mention any new activity that Maria started on 3 June, 2023.",
      "reference": "volunteering at a local dog shelter once a month",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8404672145843506,
        "bert_recall": 0.8474598526954651,
        "bert_f1": 0.8439490795135498,
        "meteor": 0.0,
        "sbert_similarity": 0.03918059542775154
      }
    },
    {
      "sample_id": 0,
      "question": "What did Maria say it was like being at the waterfall in Oregon?",
      "prediction": "I felt like I was in a fairy tale! The water sounded so calming and the surroundings were beautiful. It was truly magical!",
      "reference": "Like being in a fairy tale",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3846153846153846,
        "rouge1_f": 0.3448275862068966,
        "rouge2_f": 0.22222222222222218,
        "rougeL_f": 0.3448275862068966,
        "bleu1": 0.19230769230769232,
        "bleu2": 0.15191090506255,
        "bleu3": 0.1269751973796067,
        "bleu4": 0.09562406574442013,
        "bert_precision": 0.8499471545219421,
        "bert_recall": 0.9025895595550537,
        "bert_f1": 0.8754777312278748,
        "meteor": 0.5792207792207792,
        "sbert_similarity": 0.40294528007507324
      }
    },
    {
      "sample_id": 0,
      "question": "What does Maria say she feels when doing upside-down yoga poses?",
      "prediction": "they make me feel free and light",
      "reference": "Free and light",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6,
        "rouge1_f": 0.6,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.3779644730092272,
        "bleu3": 0.30935528891357367,
        "bleu4": 0.16348126556655487,
        "bert_precision": 0.8503856062889099,
        "bert_recall": 0.8794485330581665,
        "bert_f1": 0.8646729588508606,
        "meteor": 0.866013071895425,
        "sbert_similarity": 0.47175681591033936
      }
    },
    {
      "sample_id": 0,
      "question": "What exciting news did Maria share on 16 June, 2023?",
      "prediction": "Gonna explore more and volunteer at shelters next month. Can't wait!",
      "reference": "joined a gym",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8193142414093018,
        "bert_recall": 0.8432378172874451,
        "bert_f1": 0.8311039209365845,
        "meteor": 0.0,
        "sbert_similarity": 0.2404746115207672
      }
    },
    {
      "sample_id": 0,
      "question": "What yoga activity has Maria been trying to improve her strength and endurance?",
      "prediction": "kundalini yoga",
      "reference": "kundalini yoga",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did John recently get promoted to?",
      "prediction": "assistant manager",
      "reference": "assistant manager",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What was one of the biggest challenges John faced in his journey to becoming assistant manager?",
      "prediction": "the hard work I've put in",
      "reference": "self-doubt",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8368345499038696,
        "bert_recall": 0.8465815782546997,
        "bert_f1": 0.8416798114776611,
        "meteor": 0.0,
        "sbert_similarity": 0.18233661353588104
      }
    },
    {
      "sample_id": 0,
      "question": "How does John describe the support he received during his journey to becoming assistant manager?",
      "prediction": "it feels like all the hard work I've put in has paid off",
      "reference": "having support at home and his own grit",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8334493041038513,
        "bert_recall": 0.8371820449829102,
        "bert_f1": 0.8353114724159241,
        "meteor": 0.0,
        "sbert_similarity": 0.2335059940814972
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of event did John and his family attend in June 2023?",
      "prediction": "lively event",
      "reference": "live music event",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.8,
        "rouge2_f": 0.0,
        "rougeL_f": 0.8,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.13562437855552414,
        "bleu3": 0.10556327625176448,
        "bleu4": 0.09069748827745895,
        "bert_precision": 0.8999419212341309,
        "bert_recall": 0.8647955656051636,
        "bert_f1": 0.8820187449455261,
        "meteor": 0.3448275862068965,
        "sbert_similarity": 0.5710054636001587
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Maria need to help her cousin find a new place to live?",
      "prediction": "Things have been tough for her lately. She had to leave and find a new place in a hurry, which has been really stressful.",
      "reference": "Her cousin had to leave and find a new place in a hurry.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6470588235294118,
        "rouge1_f": 0.6486486486486487,
        "rouge2_f": 0.5714285714285714,
        "rougeL_f": 0.6486486486486487,
        "bleu1": 0.48148148148148145,
        "bleu2": 0.43033148291193524,
        "bleu3": 0.4091569047284194,
        "bleu4": 0.38609739509608965,
        "bert_precision": 0.9022003412246704,
        "bert_recall": 0.9405977129936218,
        "bert_f1": 0.9209990501403809,
        "meteor": 0.7777973155149172,
        "sbert_similarity": 0.5587724447250366
      }
    },
    {
      "sample_id": 0,
      "question": "What event did John participate in to show support for veterans' rights?",
      "prediction": "a marching event for veterans' rights",
      "reference": "marching event",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.33333333333333337,
        "rougeL_f": 0.5,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.2182178902359924,
        "bleu3": 0.1006949130683619,
        "bleu4": 0.06985342056580097,
        "bert_precision": 0.836850106716156,
        "bert_recall": 0.9024854898452759,
        "bert_f1": 0.86842942237854,
        "meteor": 0.78125,
        "sbert_similarity": 0.7472397685050964
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired John to join the marching event for veterans' rights?",
      "prediction": "It made me remember how much they sacrifice for us.",
      "reference": "Respect for the military and the desire to show support",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10526315789473685,
        "rouge1_f": 0.10000000000000002,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10000000000000002,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.8622698783874512,
        "bert_recall": 0.859320878982544,
        "bert_f1": 0.8607928156852722,
        "meteor": 0.05000000000000001,
        "sbert_similarity": 0.29547005891799927
      }
    },
    {
      "sample_id": 0,
      "question": "How often does John get to see sunsets like the one he shared with Maria?",
      "prediction": "I like to just take a look at the sunset - it reminds me to enjoy the small things in life.",
      "reference": "At least once a week",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.16000000000000003,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08000000000000002,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.020806259464411975,
        "bleu3": 0.013510398157576722,
        "bleu4": 0.010331208012220438,
        "bert_precision": 0.8431444764137268,
        "bert_recall": 0.8357223868370056,
        "bert_f1": 0.839417040348053,
        "meteor": 0.15151515151515152,
        "sbert_similarity": 0.08160547912120819
      }
    },
    {
      "sample_id": 0,
      "question": "What natural disaster affected John's old area on 7 July, 2023?",
      "prediction": "a nasty flood",
      "reference": "Flood",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.8399315476417542,
        "bert_recall": 0.9173789620399475,
        "bert_f1": 0.8769486546516418,
        "meteor": 0.4166666666666667,
        "sbert_similarity": 0.8477544784545898
      }
    },
    {
      "sample_id": 0,
      "question": "How did the flood impact the homes in John's old area?",
      "prediction": "lots of homes were ruined",
      "reference": "Lots of homes were ruined.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.8187307530779819,
        "bleu2": 0.8187307530779819,
        "bleu3": 0.8187307530779819,
        "bleu4": 0.8187307530779819,
        "bert_precision": 0.9924120903015137,
        "bert_recall": 0.9936710596084595,
        "bert_f1": 0.9930412173271179,
        "meteor": 0.7937500000000002,
        "sbert_similarity": 0.9895420074462891
      }
    },
    {
      "sample_id": 0,
      "question": "What motivated Maria and John to discuss potential solutions for their community on 7 July, 2023?",
      "prediction": "I'm getting people together to chat about this and discuss potential solutions.",
      "reference": "Flood in John's old area",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8603424429893494,
        "bert_recall": 0.8540846109390259,
        "bert_f1": 0.8572021126747131,
        "meteor": 0.0,
        "sbert_similarity": 0.0015311315655708313
      }
    },
    {
      "sample_id": 0,
      "question": "What did Maria plan to do later on the evening of 7 July, 2023?",
      "prediction": "The context does not provide information about Maria's plans for the evening of 7 July, 2023.",
      "reference": "have dinner with friends from the gym",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09090909090909091,
        "rouge1_f": 0.08333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08333333333333333,
        "bleu1": 0.05263157894736841,
        "bleu2": 0.017099639201419235,
        "bleu3": 0.012523428510015045,
        "bleu4": 0.010182425646195498,
        "bert_precision": 0.8402606248855591,
        "bert_recall": 0.863595724105835,
        "bert_f1": 0.8517683744430542,
        "meteor": 0.06329113924050632,
        "sbert_similarity": 0.05550112575292587
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of activities did Maria do at the picnic with her church friends?",
      "prediction": "chilled under the trees, played games, and ate yummy food",
      "reference": "played games like charades and a scavenger hunt",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333326,
        "rouge1_f": 0.33333333333333326,
        "rouge2_f": 0.125,
        "rougeL_f": 0.33333333333333326,
        "bleu1": 0.25,
        "bleu2": 0.1507556722888818,
        "bleu3": 0.06275677488087485,
        "bleu4": 0.03986357128268015,
        "bert_precision": 0.8495122194290161,
        "bert_recall": 0.8674278259277344,
        "bert_f1": 0.8583765625953674,
        "meteor": 0.12195121951219512,
        "sbert_similarity": 0.5248340368270874
      }
    },
    {
      "sample_id": 0,
      "question": "What does John appreciate about the veteran's hospital visit?",
      "prediction": "It made me appreciate what we have and the need to give back.",
      "reference": "the resilience of the veterans and their inspiring stories",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1904761904761905,
        "rouge1_f": 0.18181818181818185,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09090909090909093,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.033149677206589796,
        "bleu3": 0.021746357799665227,
        "bleu4": 0.0169861974906263,
        "bert_precision": 0.8743351697921753,
        "bert_recall": 0.8688740134239197,
        "bert_f1": 0.871596097946167,
        "meteor": 0.10638297872340426,
        "sbert_similarity": 0.36856311559677124
      }
    },
    {
      "sample_id": 0,
      "question": "What did John take away from visiting the veteran's hospital?",
      "prediction": "It made me appreciate what we have and the need to give back.",
      "reference": "appreciation for giving back",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352941,
        "rouge1_f": 0.3529411764705882,
        "rouge2_f": 0.13333333333333333,
        "rougeL_f": 0.3529411764705882,
        "bleu1": 0.07142857142857141,
        "bleu2": 0.023440361546924773,
        "bleu3": 0.017300021018422027,
        "bleu4": 0.01428363257865929,
        "bert_precision": 0.8752303123474121,
        "bert_recall": 0.9080379605293274,
        "bert_f1": 0.8913323879241943,
        "meteor": 0.20408163265306123,
        "sbert_similarity": 0.6938786506652832
      }
    },
    {
      "sample_id": 0,
      "question": "Why did John feel inspired to join the military after the visit to the hospital?",
      "prediction": "It reminded me why I wanted to join the military.",
      "reference": "seeing the resilience of the veterans",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.8814682364463806,
        "bert_recall": 0.8758666515350342,
        "bert_f1": 0.8786585330963135,
        "meteor": 0.078125,
        "sbert_similarity": 0.3332715630531311
      }
    },
    {
      "sample_id": 0,
      "question": "In what activity did Maria and her church friends participate in July 2023?",
      "prediction": "The provided context does not mention any activity that Maria and her church friends participated in July 2023.",
      "reference": "hiking",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7982268333435059,
        "bert_recall": 0.8222700357437134,
        "bert_f1": 0.8100700378417969,
        "meteor": 0.0,
        "sbert_similarity": 0.02263074740767479
      }
    },
    {
      "sample_id": 0,
      "question": "What does John think about trying new classes at the yoga studio?",
      "prediction": "Trying new stuff is a great way to push yourself and mix things up.",
      "reference": "Trying new classes is a fun way to switch up the exercise routine.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5185185185185186,
        "rouge1_f": 0.5185185185185186,
        "rouge2_f": 0.24000000000000002,
        "rougeL_f": 0.5185185185185186,
        "bleu1": 0.5333333333333333,
        "bleu2": 0.3380617018914066,
        "bleu3": 0.0980699685985425,
        "bleu4": 0.05202556880807584,
        "bert_precision": 0.9352785348892212,
        "bert_recall": 0.9400277137756348,
        "bert_f1": 0.9376471042633057,
        "meteor": 0.42938931297709926,
        "sbert_similarity": 0.48058876395225525
      }
    },
    {
      "sample_id": 0,
      "question": "Which activity has John done apart from yoga at the studio?",
      "prediction": "kickboxing",
      "reference": "weight training",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8501918315887451,
        "bert_recall": 0.877572774887085,
        "bert_f1": 0.8636653423309326,
        "meteor": 0.0,
        "sbert_similarity": 0.29184144735336304
      }
    },
    {
      "sample_id": 0,
      "question": "What community service did Maria mention that she was involved in on 31 July, 2023?",
      "prediction": "I'm still volunteering at the homeless shelter.",
      "reference": "volunteered at a homeless shelter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.6153846153846154,
        "rouge2_f": 0.36363636363636365,
        "rougeL_f": 0.6153846153846154,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.20412414523193154,
        "bleu3": 0.08622792534209153,
        "bleu4": 0.056122223243057295,
        "bert_precision": 0.9173321723937988,
        "bert_recall": 0.9276654124259949,
        "bert_f1": 0.9224697947502136,
        "meteor": 0.49145299145299143,
        "sbert_similarity": 0.7163556814193726
      }
    },
    {
      "sample_id": 0,
      "question": "How did Maria start volunteering at the homeless shelter?",
      "prediction": "I started volunteering here about a year ago after witnessing a family struggling on the streets.",
      "reference": "Witnessed a family struggling on the streets and reached out to the shelter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444445,
        "rouge1_f": 0.4827586206896552,
        "rouge2_f": 0.4444444444444445,
        "rougeL_f": 0.4827586206896552,
        "bleu1": 0.35294117647058826,
        "bleu2": 0.33210558207753577,
        "bleu3": 0.3123287459051282,
        "bleu4": 0.28175950490399515,
        "bert_precision": 0.903390109539032,
        "bert_recall": 0.9162790775299072,
        "bert_f1": 0.909788966178894,
        "meteor": 0.4427736006683375,
        "sbert_similarity": 0.6329364776611328
      }
    },
    {
      "sample_id": 0,
      "question": "What did John do the week before August 3, 2023 involving his kids?",
      "prediction": "Next week I have a community meeting coming up, we'll be discussing education and infrastructure upgrades.",
      "reference": "Had a meaningful experience at a military memorial",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.08695652173913043,
        "rouge1_f": 0.07999999999999999,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07999999999999999,
        "bleu1": 0.05263157894736841,
        "bleu2": 0.017099639201419235,
        "bleu3": 0.012523428510015045,
        "bleu4": 0.010182425646195498,
        "bert_precision": 0.8418475389480591,
        "bert_recall": 0.8474776744842529,
        "bert_f1": 0.8446531891822815,
        "meteor": 0.056818181818181816,
        "sbert_similarity": 0.19475656747817993
      }
    },
    {
      "sample_id": 0,
      "question": "How did John describe his kids' reaction at the military memorial?",
      "prediction": "It really made an impact on my kids.",
      "reference": "awestruck and humbled",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8604918718338013,
        "bert_recall": 0.7950894832611084,
        "bert_f1": 0.8264988660812378,
        "meteor": 0.0,
        "sbert_similarity": 0.18701618909835815
      }
    },
    {
      "sample_id": 0,
      "question": "Why does Maria think it's important for younger generations to visit military memorials?",
      "prediction": "It's important for younger generations to remember and appreciate those who served.",
      "reference": "To remember and appreciate those who served",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7368421052631579,
        "rouge1_f": 0.7000000000000001,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7000000000000001,
        "bleu1": 0.5,
        "bleu2": 0.4803844614152614,
        "bleu3": 0.46172033068712565,
        "bleu4": 0.4324227075463215,
        "bert_precision": 0.9051226377487183,
        "bert_recall": 0.9417861104011536,
        "bert_f1": 0.9230904579162598,
        "meteor": 0.7981481481481482,
        "sbert_similarity": 0.8499689102172852
      }
    },
    {
      "sample_id": 0,
      "question": "What does John believe is important for children regarding veterans?",
      "prediction": "It's super important to teach kids about veterans and what they did for us.",
      "reference": "Teaching them to respect and appreciate those who served",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.17391304347826086,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666669,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.028867513459481287,
        "bleu3": 0.018864614597621485,
        "bleu4": 0.014628063653657535,
        "bert_precision": 0.8790656924247742,
        "bert_recall": 0.8890218734741211,
        "bert_f1": 0.8840157389640808,
        "meteor": 0.15789473684210528,
        "sbert_similarity": 0.6113033294677734
      }
    },
    {
      "sample_id": 0,
      "question": "What happened to John's job in August 2023?",
      "prediction": "I lost my job at the mechanical engineering company. They tanked and it's been really rough.",
      "reference": "John lost his job at the mechanical engineering company.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.56,
        "rouge1_f": 0.5384615384615384,
        "rouge2_f": 0.4166666666666667,
        "rougeL_f": 0.5384615384615384,
        "bleu1": 0.42105263157894735,
        "bleu2": 0.37463432463267754,
        "bleu3": 0.34929546789787863,
        "bleu4": 0.3187271473320672,
        "bert_precision": 0.9132847785949707,
        "bert_recall": 0.9739330410957336,
        "bert_f1": 0.9426344037055969,
        "meteor": 0.7132337471070903,
        "sbert_similarity": 0.5665500164031982
      }
    },
    {
      "sample_id": 0,
      "question": "What activity did Maria take up with her friends from church in August 2023?",
      "prediction": "Enjoy your family time, John! Have a great day!",
      "reference": "community work",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8112905025482178,
        "bert_recall": 0.8267468810081482,
        "bert_f1": 0.8189457654953003,
        "meteor": 0.0,
        "sbert_similarity": 0.12003419548273087
      }
    },
    {
      "sample_id": 0,
      "question": "What did John do to help his community last year in his hometown?",
      "prediction": "participated in a community event to raise money for a good cause",
      "reference": "Helped renovate a rundown community center.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.23529411764705885,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.03892494720807617,
        "bleu3": 0.025677367760996977,
        "bleu4": 0.020255986027125642,
        "bert_precision": 0.8740711212158203,
        "bert_recall": 0.8784639239311218,
        "bert_f1": 0.8762620091438293,
        "meteor": 0.15151515151515152,
        "sbert_similarity": 0.45423778891563416
      }
    },
    {
      "sample_id": 0,
      "question": "What cause did the 5K charity run organized by John support?",
      "prediction": "to help out veterans and their families",
      "reference": "veterans and their families",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7272727272727273,
        "rouge1_f": 0.7272727272727273,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7272727272727273,
        "bleu1": 0.5714285714285714,
        "bleu2": 0.5345224838248488,
        "bleu3": 0.4888069165703076,
        "bleu4": 0.41113361690051975,
        "bert_precision": 0.878667950630188,
        "bert_recall": 0.9126772880554199,
        "bert_f1": 0.8953497409820557,
        "meteor": 0.9229651162790699,
        "sbert_similarity": 0.7708548307418823
      }
    },
    {
      "sample_id": 0,
      "question": "Who did John work with to raise awareness and funds for victims of domestic abuse?",
      "prediction": "a local organization that helps victims of domestic abuse",
      "reference": "a local organization",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.28867513459481287,
        "bleu3": 0.23173297416566233,
        "bleu4": 0.11868405219520975,
        "bert_precision": 0.8671517372131348,
        "bert_recall": 0.9622970819473267,
        "bert_f1": 0.912250280380249,
        "meteor": 0.8179012345679013,
        "sbert_similarity": 0.5425670742988586
      }
    },
    {
      "sample_id": 0,
      "question": "What recognition did Maria receive at the homeless shelter in August 2023?",
      "prediction": "they gave me a medal",
      "reference": "a medal for volunteering",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444445,
        "rouge1_f": 0.4444444444444445,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.4444444444444445,
        "bleu1": 0.4,
        "bleu2": 0.316227766016838,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.8919216394424438,
        "bert_recall": 0.893650472164154,
        "bert_f1": 0.8927852511405945,
        "meteor": 0.45731707317073167,
        "sbert_similarity": 0.5426209568977356
      }
    },
    {
      "sample_id": 0,
      "question": "What is the name of Maria's puppy she got two weeks before August 11, 2023?",
      "prediction": "Coco",
      "reference": "Coco",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What activity did John and Max enjoy together last summer?",
      "prediction": "camping trip",
      "reference": "Camping",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.8818403482437134,
        "bert_recall": 0.9726747274398804,
        "bert_f1": 0.9250329732894897,
        "meteor": 0.45454545454545453,
        "sbert_similarity": 0.9117497205734253
      }
    },
    {
      "sample_id": 0,
      "question": "How does John describe the camping trip with Max?",
      "prediction": "We hiked, swam, and made great memories. It was a really peaceful and awesome experience.",
      "reference": "Peaceful and awesome",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.35294117647058826,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.25,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.15789473684210523,
        "bleu2": 0.13245323570650439,
        "bleu3": 0.10339824169628065,
        "bleu4": 0.05039518688486959,
        "bert_precision": 0.8504082560539246,
        "bert_recall": 0.861443042755127,
        "bert_f1": 0.8558900356292725,
        "meteor": 0.701058201058201,
        "sbert_similarity": 0.37700048089027405
      }
    },
    {
      "sample_id": 0,
      "question": "Why does John say he feels stuck and questioning his decisions and goals?",
      "prediction": "Don't feel like I'm making much of an impact here, which has me questioning my decisions and goals.",
      "reference": "Not feeling like making much of an impact",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.46153846153846156,
        "rouge1_f": 0.4999999999999999,
        "rouge2_f": 0.3846153846153846,
        "rougeL_f": 0.4999999999999999,
        "bleu1": 0.2727272727272727,
        "bleu2": 0.2279211529192759,
        "bleu3": 0.20148743737862052,
        "bleu4": 0.16923267918690046,
        "bert_precision": 0.9046393036842346,
        "bert_recall": 0.9276735186576843,
        "bert_f1": 0.9160116314888,
        "meteor": 0.7687074829931974,
        "sbert_similarity": 0.5911235809326172
      }
    },
    {
      "sample_id": 0,
      "question": "What is the name of Maria's second puppy?",
      "prediction": "Shadow",
      "reference": "Shadow",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "How is Maria's new puppy adjusting to its new home?",
      "prediction": "It's definitely been an adjustment taking care of her, but it's totally worth it!",
      "reference": "doing great - learning commands and house training",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8199029564857483,
        "bert_recall": 0.840095043182373,
        "bert_f1": 0.829876184463501,
        "meteor": 0.058139534883720936,
        "sbert_similarity": 0.10306272655725479
      }
    },
    {
      "sample_id": 0,
      "question": "What is John currently doing as a volunteer in August 2023?",
      "prediction": "volunteering at a homeless shelter",
      "reference": "mentoring students at a local school",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3636363636363636,
        "rouge1_f": 0.3636363636363636,
        "rouge2_f": 0.22222222222222224,
        "rougeL_f": 0.3636363636363636,
        "bleu1": 0.3274923012311928,
        "bleu2": 0.25890539701513365,
        "bleu3": 0.12464965750914189,
        "bleu4": 0.09302577375697234,
        "bert_precision": 0.8919777870178223,
        "bert_recall": 0.8776199817657471,
        "bert_f1": 0.8847406506538391,
        "meteor": 0.3177966101694915,
        "sbert_similarity": 0.36855003237724304
      }
    },
    {
      "sample_id": 0,
      "question": "What activities does John's family enjoy doing together?",
      "prediction": "a walk or picnic in the park, or finding events in our town and beyond",
      "reference": "going for hikes, hanging out at the park, having picnics, playing board games, having movie nights",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714288,
        "rouge1_f": 0.19354838709677422,
        "rouge2_f": 0.0689655172413793,
        "rougeL_f": 0.12903225806451615,
        "bleu1": 0.1460251468258884,
        "bleu2": 0.12313921590141719,
        "bleu3": 0.09649933724090917,
        "bleu4": 0.04741257412575257,
        "bert_precision": 0.8810154795646667,
        "bert_recall": 0.8553716540336609,
        "bert_f1": 0.8680042028427124,
        "meteor": 0.1179245283018868,
        "sbert_similarity": 0.52179354429245
      }
    },
    {
      "sample_id": 0,
      "question": "What did the donations help John's community acquire on 16 August, 2023?",
      "prediction": "to help out veterans and their families.",
      "reference": "a brand new fire truck",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8588905334472656,
        "bert_recall": 0.8557033538818359,
        "bert_f1": 0.857293963432312,
        "meteor": 0.0,
        "sbert_similarity": 0.0954388678073883
      }
    },
    {
      "sample_id": 0,
      "question": "What is John's main focus in international politics?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Improving education and infrastructure",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8677957057952881,
        "bert_recall": 0.8312066197395325,
        "bert_f1": 0.8491072058677673,
        "meteor": 0.0,
        "sbert_similarity": 0.026223765686154366
      }
    },
    {
      "sample_id": 0,
      "question": "What did Maria donate to a luxury store in December 2023?",
      "prediction": "Not mentioned in the conversation",
      "reference": "old car",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8734785318374634,
        "bert_recall": 0.8881524205207825,
        "bert_f1": 0.880754292011261,
        "meteor": 0.0,
        "sbert_similarity": 0.08534565567970276
      }
    },
    {
      "sample_id": 0,
      "question": "Who inspired John to start volunteering?",
      "prediction": "Not mentioned in the conversation",
      "reference": "His aunt",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8780044317245483,
        "bert_recall": 0.8678624033927917,
        "bert_f1": 0.8729040026664734,
        "meteor": 0.0,
        "sbert_similarity": 0.20736488699913025
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Maria decide to run for office again?",
      "prediction": "Not mentioned in the conversation",
      "reference": "saw the impact she could make in the community through politics",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.26666666666666666,
        "rouge1_f": 0.25000000000000006,
        "rouge2_f": 0.14285714285714288,
        "rougeL_f": 0.25000000000000006,
        "bleu1": 0.12047768476488081,
        "bleu2": 0.09524597277019774,
        "bleu3": 0.045856046346674784,
        "bleu4": 0.034222269664256,
        "bert_precision": 0.8886204957962036,
        "bert_recall": 0.8568218946456909,
        "bert_f1": 0.8724315166473389,
        "meteor": 0.18028846153846154,
        "sbert_similarity": 0.13625279068946838
      }
    },
    {
      "sample_id": 0,
      "question": "What activity did Maria's colleague, Rob, invite her to?",
      "prediction": "Not mentioned in the conversation",
      "reference": "beginner's yoga class",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8674252033233643,
        "bert_recall": 0.8361620903015137,
        "bert_f1": 0.851506769657135,
        "meteor": 0.0,
        "sbert_similarity": -0.013614021241664886
      }
    },
    {
      "sample_id": 0,
      "question": "What is the name of Maria's one-year-old child?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Kyle",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8487465381622314,
        "bert_recall": 0.8953734636306763,
        "bert_f1": 0.8714367747306824,
        "meteor": 0.0,
        "sbert_similarity": 0.12340737879276276
      }
    },
    {
      "sample_id": 0,
      "question": "How often does John take his kids to the library?",
      "prediction": "Not mentioned in the conversation",
      "reference": "A few times a week",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8710613250732422,
        "bert_recall": 0.8341257572174072,
        "bert_f1": 0.8521934747695923,
        "meteor": 0.0,
        "sbert_similarity": 0.12815101444721222
      }
    },
    {
      "sample_id": 0,
      "question": "What did Maria make for her home to remind her of a trip to France?",
      "prediction": "Not mentioned in the conversation",
      "reference": "painting of a castle on a hill",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8793039917945862,
        "bert_recall": 0.8493489027023315,
        "bert_f1": 0.8640668988227844,
        "meteor": 0.0,
        "sbert_similarity": 0.015213102102279663
      }
    },
    {
      "sample_id": 0,
      "question": "Where did John get the idea for the castle shadow box in his home?",
      "prediction": "England",
      "reference": "England",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did Maria receive a certificate for?",
      "prediction": "Not mentioned in the conversation",
      "reference": "completion of a university degree",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8716192841529846,
        "bert_recall": 0.8446286916732788,
        "bert_f1": 0.8579117655754089,
        "meteor": 0.0,
        "sbert_similarity": -0.0025042016059160233
      }
    },
    {
      "sample_id": 0,
      "question": "What areas is John particularly interested in for art appreciation?",
      "prediction": "Not mentioned in the conversation",
      "reference": "education and infrastructure",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8672285079956055,
        "bert_recall": 0.8562148809432983,
        "bert_f1": 0.8616865277290344,
        "meteor": 0.0,
        "sbert_similarity": 0.0783056989312172
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Maria start blogging about politics and policies?",
      "prediction": "raise awareness and start conversations to create positive change",
      "reference": "raise awareness and start conversations to create positive change",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 0,
      "question": "What was the focus of John's recent travel and photography blog?",
      "prediction": "Not mentioned in the conversation",
      "reference": "education reform and infrastructure development",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8737524747848511,
        "bert_recall": 0.8355357646942139,
        "bert_f1": 0.8542169332504272,
        "meteor": 0.0,
        "sbert_similarity": -0.007084940560162067
      }
    },
    {
      "sample_id": 0,
      "question": "How often does Maria work out with her family?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Three times a week",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8668030500411987,
        "bert_recall": 0.8225946426391602,
        "bert_f1": 0.8441203832626343,
        "meteor": 0.0,
        "sbert_similarity": 0.07513658702373505
      }
    },
    {
      "sample_id": 0,
      "question": "How has John's artistic skills improved since starting boot camps with his family?",
      "prediction": "Not mentioned in the conversation",
      "reference": "More energy, gains in strength and endurance",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.10976232721880529,
        "bleu2": 0.03880684294761699,
        "bleu3": 0.03109090746822916,
        "bleu4": 0.029486824119076216,
        "bert_precision": 0.8746093511581421,
        "bert_recall": 0.8457319736480713,
        "bert_f1": 0.85992830991745,
        "meteor": 0.07352941176470587,
        "sbert_similarity": 0.053643904626369476
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of food did Maria have on her dinner spread with her father?",
      "prediction": "Salads, sandwiches, homemade desserts",
      "reference": "Salads, sandwiches, homemade desserts",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What did John do to feel closer to a community and his faith?",
      "prediction": "joined a nearby church",
      "reference": "joined a nearby church",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "Why did John join a nearby church recently?",
      "prediction": "Not mentioned in the conversation",
      "reference": "to feel closer to a community and her faith",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8722929954528809,
        "bert_recall": 0.8367729187011719,
        "bert_f1": 0.8541638255119324,
        "meteor": 0.0,
        "sbert_similarity": 0.1263028085231781
      }
    },
    {
      "sample_id": 0,
      "question": "How long was Max a part of Maria's family?",
      "prediction": "10 years",
      "reference": "10 years",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "How does Maria plan to honor the memories of her beloved pet?",
      "prediction": "Not mentioned in the conversation",
      "reference": "By considering adopting a rescue dog",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8789246082305908,
        "bert_recall": 0.8439451456069946,
        "bert_f1": 0.861079752445221,
        "meteor": 0.0,
        "sbert_similarity": 0.0012524882331490517
      }
    },
    {
      "sample_id": 0,
      "question": "What important values does Maria want to teach her kids through adopting a rescue dog?",
      "prediction": "Responsibility and compassion",
      "reference": "Responsibility and compassion",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did Maria say it was like being at the desert in Oregon?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Like being in a fairy tale",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1818181818181818,
        "rouge1_f": 0.1818181818181818,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1818181818181818,
        "bleu1": 0.1637461506155964,
        "bleu2": 0.05789300674674099,
        "bleu3": 0.046382183633183686,
        "bleu4": 0.0439891724758422,
        "bert_precision": 0.8761690855026245,
        "bert_recall": 0.8417006731033325,
        "bert_f1": 0.8585891127586365,
        "meteor": 0.0847457627118644,
        "sbert_similarity": 0.1141383945941925
      }
    },
    {
      "sample_id": 0,
      "question": "What does John say she feels when doing upside-down yoga poses?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Free and light",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8751981258392334,
        "bert_recall": 0.8313731551170349,
        "bert_f1": 0.8527229428291321,
        "meteor": 0.0,
        "sbert_similarity": 0.13863016664981842
      }
    },
    {
      "sample_id": 0,
      "question": "What did Maria recently get promoted to?",
      "prediction": "Not mentioned in the conversation",
      "reference": "assistant manager",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8727396726608276,
        "bert_recall": 0.8772444128990173,
        "bert_f1": 0.8749862313270569,
        "meteor": 0.0,
        "sbert_similarity": 0.0999579131603241
      }
    },
    {
      "sample_id": 0,
      "question": "What was one of the biggest challenges Maria faced in her journey to becoming assistant manager?",
      "prediction": "self-doubt",
      "reference": "self-doubt",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 0,
      "question": "Why did John need to help his cousin find a new place to live?",
      "prediction": "His cousin had to leave and find a new place in a hurry.",
      "reference": "His cousin had to leave and find a new place in a hurry.",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9997724169321802,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What event did Maria participate in to show support for veterans' rights?",
      "prediction": "marching event",
      "reference": "marching event",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.000000238418579,
        "bert_recall": 1.000000238418579,
        "bert_f1": 1.000000238418579,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "How did the drought impact the homes in John's old area?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Lots of homes were ruined.",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8776593208312988,
        "bert_recall": 0.8685310482978821,
        "bert_f1": 0.8730713725090027,
        "meteor": 0.0,
        "sbert_similarity": 0.03102242201566696
      }
    },
    {
      "sample_id": 0,
      "question": "What does John criticize about the veteran's hospital visit?",
      "prediction": "Not mentioned in the conversation",
      "reference": "the resilience of the veterans and their inspiring stories",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615385,
        "rouge1_f": 0.14285714285714285,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.08986579282344431,
        "bleu2": 0.03177235575108143,
        "bleu3": 0.02545508208534111,
        "bleu4": 0.02414176971688927,
        "bert_precision": 0.8857161998748779,
        "bert_recall": 0.862253725528717,
        "bert_f1": 0.8738275170326233,
        "meteor": 0.05813953488372093,
        "sbert_similarity": -0.023380642756819725
      }
    },
    {
      "sample_id": 0,
      "question": "What did John take away from visiting the orphanage?",
      "prediction": "appreciation for giving back",
      "reference": "appreciation for giving back",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Maria feel inspired to join the military after the visit to the hospital?",
      "prediction": "seeing the resilience of the veterans",
      "reference": "seeing the resilience of the veterans",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9976851851851852,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "How did Maria describe her kids' reaction at the military memorial?",
      "prediction": "Not mentioned in the conversation",
      "reference": "awestruck and humbled",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8673855066299438,
        "bert_recall": 0.7843987941741943,
        "bert_f1": 0.8238075375556946,
        "meteor": 0.0,
        "sbert_similarity": 0.032112836837768555
      }
    },
    {
      "sample_id": 0,
      "question": "Why does Maria think it's important for younger generations to visit art galleries?",
      "prediction": "Not mentioned in the conversation",
      "reference": "To remember and appreciate those who served",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8674697875976562,
        "bert_recall": 0.834204912185669,
        "bert_f1": 0.8505122065544128,
        "meteor": 0.0,
        "sbert_similarity": 0.10402170568704605
      }
    },
    {
      "sample_id": 0,
      "question": "What happened to Maria's job in August 2023?",
      "prediction": "Not mentioned in the conversation",
      "reference": "John lost his job at the mechanical engineering company.",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714285,
        "rouge1_f": 0.14285714285714285,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.07357588823428847,
        "bleu2": 0.02601300475114445,
        "bleu3": 0.020840858525393174,
        "bleu4": 0.01976560930094397,
        "bert_precision": 0.8863264918327332,
        "bert_recall": 0.8715265393257141,
        "bert_f1": 0.8788642287254333,
        "meteor": 0.05813953488372093,
        "sbert_similarity": 0.046728283166885376
      }
    },
    {
      "sample_id": 0,
      "question": "What cause did the 5K charity run organized by Maria support?",
      "prediction": "Not mentioned in the conversation",
      "reference": "veterans and their families",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8808323740959167,
        "bert_recall": 0.8529139757156372,
        "bert_f1": 0.8666483759880066,
        "meteor": 0.0,
        "sbert_similarity": 0.0780658945441246
      }
    },
    {
      "sample_id": 0,
      "question": "Who did John work with to raise awareness and funds for animal welfare?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a local organization",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8729597330093384,
        "bert_recall": 0.8615210056304932,
        "bert_f1": 0.8672026991844177,
        "meteor": 0.0,
        "sbert_similarity": 0.05029366910457611
      }
    },
    {
      "sample_id": 0,
      "question": "What recognition did John receive at the homeless shelter in August 2023?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a medal for volunteering",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8825687170028687,
        "bert_recall": 0.8524521589279175,
        "bert_f1": 0.8672490119934082,
        "meteor": 0.0,
        "sbert_similarity": 0.020397096872329712
      }
    },
    {
      "sample_id": 0,
      "question": "What is the name of John's puppy he got two weeks before August 11, 2023?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Coco",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.848440945148468,
        "bert_recall": 0.8946402072906494,
        "bert_f1": 0.8709283471107483,
        "meteor": 0.0,
        "sbert_similarity": 0.12292777001857758
      }
    },
    {
      "sample_id": 0,
      "question": "How does Maria describe the camping trip with Max?",
      "prediction": "Peaceful and awesome",
      "reference": "Peaceful and awesome",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What is the name of Maria's second kitten?",
      "prediction": "Shadow",
      "reference": "Shadow",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "How is John's new puppy adjusting to its new home?",
      "prediction": "doing great - learning commands and house training",
      "reference": "doing great - learning commands and house training",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9990234375,
        "sbert_similarity": 1.0
      }
    }
  ]
}