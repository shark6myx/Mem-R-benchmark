{
  "sample_idx": 6,
  "total_questions": 190,
  "individual_results": [
    {
      "sample_id": 0,
      "question": "What are John's suspected health problems?",
      "prediction": "The content does not provide any information about John's suspected health problems.",
      "reference": "Obesity",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7861936688423157,
        "bert_recall": 0.8109354972839355,
        "bert_f1": 0.7983729839324951,
        "meteor": 0.0,
        "sbert_similarity": 0.06976589560508728
      }
    },
    {
      "sample_id": 0,
      "question": "Which recreational activity was James pursuing on March 16, 2022?",
      "prediction": "bowling",
      "reference": "bowling",
      "category": 2,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "Which places or events have John and James planned to meet at?",
      "prediction": "Starbucks for coffee",
      "reference": "VR Club, McGee's, baseball game",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8108577728271484,
        "bert_recall": 0.7752869129180908,
        "bert_f1": 0.7926735281944275,
        "meteor": 0.0,
        "sbert_similarity": 0.20112335681915283
      }
    },
    {
      "sample_id": 0,
      "question": "Do both James and John have pets?",
      "prediction": "Yes, James has dogs and John appreciates dogs.",
      "reference": "No",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8297713994979858,
        "bert_recall": 0.8549728989601135,
        "bert_f1": 0.8421836495399475,
        "meteor": 0.0,
        "sbert_similarity": 0.17370401322841644
      }
    },
    {
      "sample_id": 0,
      "question": "When did John resume playing drums in his adulthood?",
      "prediction": "No specific date mentioned, but John reflected on playing drums when he was younger.",
      "reference": "February 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.830254077911377,
        "bert_recall": 0.8356226086616516,
        "bert_f1": 0.8329296708106995,
        "meteor": 0.0,
        "sbert_similarity": 0.15731246769428253
      }
    },
    {
      "sample_id": 0,
      "question": "What are John and James' favorite games?",
      "prediction": "AC Valhalla, Apex Legends, and strategy games like Civilization and Total War.",
      "reference": "John's favorite game is CS:GO, and James's is Apex Legends.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3,
        "rouge1_f": 0.32,
        "rouge2_f": 0.08695652173913043,
        "rougeL_f": 0.16,
        "bleu1": 0.3118356616772059,
        "bleu2": 0.2041443605746337,
        "bleu3": 0.0687247804353215,
        "bleu4": 0.039103088536506424,
        "bert_precision": 0.8728476762771606,
        "bert_recall": 0.8735675811767578,
        "bert_f1": 0.8732074499130249,
        "meteor": 0.14705882352941177,
        "sbert_similarity": 0.44672489166259766
      }
    },
    {
      "sample_id": 0,
      "question": "Does James live in Connecticut?",
      "prediction": "The context does not provide information about James's location.",
      "reference": "Likely yes",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8149621486663818,
        "bert_recall": 0.8166114687919617,
        "bert_f1": 0.8157859444618225,
        "meteor": 0.0,
        "sbert_similarity": -0.003035210072994232
      }
    },
    {
      "sample_id": 0,
      "question": "In which state is the shelter from which James adopted the puppy?",
      "prediction": "Stamford",
      "reference": "Connecticut.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9428826570510864,
        "bert_recall": 0.9714095592498779,
        "bert_f1": 0.9569335579872131,
        "meteor": 0.0,
        "sbert_similarity": 0.4885808229446411
      }
    },
    {
      "sample_id": 0,
      "question": "How many pets does James have?",
      "prediction": "three dogs",
      "reference": "Three dogs.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.6065306597126334,
        "bleu2": 0.6065306597126334,
        "bleu3": 0.28369570381570614,
        "bleu4": 0.19180183554164504,
        "bert_precision": 0.973300576210022,
        "bert_recall": 0.9821858406066895,
        "bert_f1": 0.9777230620384216,
        "meteor": 0.25,
        "sbert_similarity": 0.993301510810852
      }
    },
    {
      "sample_id": 0,
      "question": "What are the names of James's dogs?",
      "prediction": "Max and two dogs (name not specified)",
      "reference": "Ned, Daisy, Max",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.8422149419784546,
        "bert_recall": 0.8624074459075928,
        "bert_f1": 0.8521915674209595,
        "meteor": 0.14705882352941177,
        "sbert_similarity": 0.4781711995601654
      }
    },
    {
      "sample_id": 0,
      "question": "When did James adopt Ned?",
      "prediction": "last week on 12 April, 2022",
      "reference": "first week of April 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.5454545454545454,
        "rouge2_f": 0.22222222222222224,
        "rougeL_f": 0.5454545454545454,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.08451542547285168,
        "bleu3": 0.053841561511499283,
        "bleu4": 0.043472087194499145,
        "bert_precision": 0.8974198698997498,
        "bert_recall": 0.9238961935043335,
        "bert_f1": 0.9104655981063843,
        "meteor": 0.19607843137254904,
        "sbert_similarity": 0.7751233577728271
      }
    },
    {
      "sample_id": 0,
      "question": "How was John feeling on April 10, 2022?",
      "prediction": "Excited to hear thoughts.",
      "reference": "seeking solitude",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8307162523269653,
        "bert_recall": 0.8528388738632202,
        "bert_f1": 0.8416322469711304,
        "meteor": 0.0,
        "sbert_similarity": 0.2224569171667099
      }
    },
    {
      "sample_id": 0,
      "question": "Did James have a girlfriend during April 2022?",
      "prediction": "No, he asked Samantha to become his girlfriend in September 2022.",
      "reference": "Presumably not",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8245581984519958,
        "bert_recall": 0.8207060098648071,
        "bert_f1": 0.822627604007721,
        "meteor": 0.0,
        "sbert_similarity": 0.28934064507484436
      }
    },
    {
      "sample_id": 0,
      "question": "When did James visit Italy?",
      "prediction": "Last year.",
      "reference": "In 2021",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9108022451400757,
        "bert_recall": 0.8662469387054443,
        "bert_f1": 0.8879660367965698,
        "meteor": 0.0,
        "sbert_similarity": 0.46879512071609497
      }
    },
    {
      "sample_id": 0,
      "question": "When did James buy himself a new adventure book?",
      "prediction": "three days ago from 29 April, 2022",
      "reference": "April 26, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.375,
        "bleu2": 0.23145502494313785,
        "bleu3": 0.098573017775554,
        "bleu4": 0.06500593260343691,
        "bert_precision": 0.8176429271697998,
        "bert_recall": 0.8737029433250427,
        "bert_f1": 0.8447438478469849,
        "meteor": 0.14705882352941177,
        "sbert_similarity": 0.7375664710998535
      }
    },
    {
      "sample_id": 0,
      "question": "When did James start playing Civilization VI?",
      "prediction": "29 April, 2022",
      "reference": "March 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.8603184223175049,
        "bert_recall": 0.9196757674217224,
        "bert_f1": 0.8890073299407959,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.8125169277191162
      }
    },
    {
      "sample_id": 0,
      "question": "What is the game with different colored cards that was John talking about with James?",
      "prediction": "a card game with multi-colored cards with numbers",
      "reference": "UNO",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.802764892578125,
        "bert_recall": 0.8242871761322021,
        "bert_f1": 0.8133836984634399,
        "meteor": 0.0,
        "sbert_similarity": 0.2514987885951996
      }
    },
    {
      "sample_id": 0,
      "question": "What is the board game where you have to find the imposter that John mentions to James?",
      "prediction": "a game to figure out who the impostors are",
      "reference": "Mafia",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7939574122428894,
        "bert_recall": 0.855364978313446,
        "bert_f1": 0.8235179781913757,
        "meteor": 0.0,
        "sbert_similarity": 0.2944098711013794
      }
    },
    {
      "sample_id": 0,
      "question": "Which books has John recommended to James?",
      "prediction": "\"The Stormlight Archive\", \"Kingkiller Chronicle\", and \"The Expanse\" series.",
      "reference": "The Name of the Wind, Stormlight Archive, Kingkiller Chronicles, Expanse",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352941,
        "rouge1_f": 0.7368421052631577,
        "rouge2_f": 0.35294117647058826,
        "rougeL_f": 0.631578947368421,
        "bleu1": 0.4444444444444444,
        "bleu2": 0.16169041669088866,
        "bleu3": 0.056282325133473234,
        "bleu4": 0.03230645681297444,
        "bert_precision": 0.9001690745353699,
        "bert_recall": 0.8940150141716003,
        "bert_f1": 0.8970814943313599,
        "meteor": 0.05050505050505051,
        "sbert_similarity": 0.8133702278137207
      }
    },
    {
      "sample_id": 0,
      "question": "Was James feeling lonely before meeting Samantha?",
      "prediction": "I'm lucky to have at least two people who always help me out when I'm struggling.",
      "reference": "Most likely yes, because he mentioned that the only creatures that gave him joy are dogs and he was actively trying to date.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.05555555555555555,
        "rouge1_f": 0.04878048780487805,
        "rouge2_f": 0.0,
        "rougeL_f": 0.04878048780487805,
        "bleu1": 0.07675925816055104,
        "bleu2": 0.017634222610988584,
        "bleu3": 0.011479355712792363,
        "bleu4": 0.008830049141178208,
        "bert_precision": 0.864863932132721,
        "bert_recall": 0.8569846153259277,
        "bert_f1": 0.8609062433242798,
        "meteor": 0.02242152466367713,
        "sbert_similarity": 0.022474125027656555
      }
    },
    {
      "sample_id": 0,
      "question": "How many charity tournaments has John organized till date?",
      "prediction": "a gaming tourney with my buddies last night.",
      "reference": "two",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8158130645751953,
        "bert_recall": 0.8405225872993469,
        "bert_f1": 0.827983558177948,
        "meteor": 0.0,
        "sbert_similarity": 0.22677850723266602
      }
    },
    {
      "sample_id": 0,
      "question": "When did John first organize a charity tournament with his friends?",
      "prediction": "8 May, 2022",
      "reference": "May 7, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.75,
        "bleu2": 0.49999999999999994,
        "bleu3": 0.23549424823785386,
        "bleu4": 0.1880301546543197,
        "bert_precision": 0.9418185949325562,
        "bert_recall": 0.9418185949325562,
        "bert_f1": 0.9418185949325562,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.9135408997535706
      }
    },
    {
      "sample_id": 0,
      "question": "Who or which organizations have been the beneficiaries of John's charity tournaments?",
      "prediction": "a children's hospital and a dog shelter",
      "reference": "animal shelter, homeless, children's hospital",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.3333333333333333,
        "rougeL_f": 0.42857142857142855,
        "bleu1": 0.5,
        "bleu2": 0.3779644730092272,
        "bleu3": 0.29129144784395894,
        "bleu4": 0.1477219991186121,
        "bert_precision": 0.8891465663909912,
        "bert_recall": 0.8610588908195496,
        "bert_f1": 0.8748772740364075,
        "meteor": 0.3605769230769231,
        "sbert_similarity": 0.7951693534851074
      }
    },
    {
      "sample_id": 0,
      "question": "When will John start his new job?",
      "prediction": "next month",
      "reference": "In July, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8711369037628174,
        "bert_recall": 0.8226996660232544,
        "bert_f1": 0.8462257385253906,
        "meteor": 0.0,
        "sbert_similarity": 0.561346173286438
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of games has James tried to develop?",
      "prediction": "different genres and strategy games like Civilization",
      "reference": "football simulator, virtual world inspired by Witcher 3",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8523648977279663,
        "bert_recall": 0.8507754802703857,
        "bert_f1": 0.851569414138794,
        "meteor": 0.0,
        "sbert_similarity": 0.3602176606655121
      }
    },
    {
      "sample_id": 0,
      "question": "Are John and James fans of the same football team?",
      "prediction": "No, John is a Manchester City fan and James supports Liverpool.",
      "reference": "No, James is a Liverpool fan and John is a Manchester City fan.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.9523809523809523,
        "rouge1_f": 0.8333333333333333,
        "rouge2_f": 0.5454545454545454,
        "rougeL_f": 0.5833333333333334,
        "bleu1": 0.7914497715327151,
        "bleu2": 0.6291624691812853,
        "bleu3": 0.5005952475816434,
        "bleu4": 0.42210575800873296,
        "bert_precision": 0.9727945327758789,
        "bert_recall": 0.9762458801269531,
        "bert_f1": 0.9745171666145325,
        "meteor": 0.6722608024691359,
        "sbert_similarity": 0.9272546768188477
      }
    },
    {
      "sample_id": 0,
      "question": "Which countries has James visited?",
      "prediction": "Italy, Turkey, and Mexico.",
      "reference": "Italy, Mexico, Turkey, Canada, Greenland",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666665,
        "rouge1_f": 0.6666666666666665,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4444444444444445,
        "bleu1": 0.5367694950537757,
        "bleu2": 0.4490935799743743,
        "bleu3": 0.31455283992081945,
        "bleu4": 0.15447878876032708,
        "bert_precision": 0.9555916786193848,
        "bert_recall": 0.9038059711456299,
        "bert_f1": 0.9289776682853699,
        "meteor": 0.20408163265306123,
        "sbert_similarity": 0.8088340759277344
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of classes has James joined?",
      "prediction": "programming course",
      "reference": "game design course, cooking classes",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.06766764161830635,
        "bleu2": 0.030261889307125377,
        "bleu3": 0.02355435073584915,
        "bleu4": 0.020237345084409754,
        "bert_precision": 0.9338206052780151,
        "bert_recall": 0.8668358325958252,
        "bert_f1": 0.8990823030471802,
        "meteor": 0.0,
        "sbert_similarity": 0.4837924838066101
      }
    },
    {
      "sample_id": 0,
      "question": "When did James volunteer at an organization?",
      "prediction": "last month",
      "reference": "May 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8850929737091064,
        "bert_recall": 0.8959164619445801,
        "bert_f1": 0.8904718160629272,
        "meteor": 0.0,
        "sbert_similarity": 0.39935460686683655
      }
    },
    {
      "sample_id": 0,
      "question": "When did James depart for his trip to Canada?",
      "prediction": "the day after tomorrow evening from 5:13 pm on 9 July, 2022",
      "reference": "July 11, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.26666666666666666,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.23076923076923078,
        "bleu2": 0.1386750490563073,
        "bleu3": 0.057551852907951895,
        "bleu4": 0.036362270465000714,
        "bert_precision": 0.8004554510116577,
        "bert_recall": 0.8846872448921204,
        "bert_f1": 0.8404661417007446,
        "meteor": 0.12820512820512822,
        "sbert_similarity": 0.6890960931777954
      }
    },
    {
      "sample_id": 0,
      "question": "Which country did James book tickets for in July 2022?",
      "prediction": "Toronto",
      "reference": "Canada",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9995255470275879,
        "bert_recall": 0.9995255470275879,
        "bert_f1": 0.9995255470275879,
        "meteor": 0.0,
        "sbert_similarity": 0.706324577331543
      }
    },
    {
      "sample_id": 0,
      "question": "How many days did James plan to spend on his trip in Canada?",
      "prediction": "leaving the day after tomorrow evening",
      "reference": "19 days",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7875442504882812,
        "bert_recall": 0.8610151410102844,
        "bert_f1": 0.8226425647735596,
        "meteor": 0.20833333333333334,
        "sbert_similarity": 0.31931182742118835
      }
    },
    {
      "sample_id": 0,
      "question": "Where was James at on July 12, 2022?",
      "prediction": "Toronto",
      "reference": "Toronto, Canada",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.8896604180335999,
        "bert_recall": 0.8057218790054321,
        "bert_f1": 0.8456132411956787,
        "meteor": 0.0,
        "sbert_similarity": 0.881628155708313
      }
    },
    {
      "sample_id": 0,
      "question": "Did John and James study together?",
      "prediction": "I have great news! Last Tuesday I met three cool new friends in my programming course, they share the same passion as me.",
      "reference": "Yes",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8046269416809082,
        "bert_recall": 0.8173102140426636,
        "bert_f1": 0.8109190464019775,
        "meteor": 0.0,
        "sbert_similarity": 0.1125316172838211
      }
    },
    {
      "sample_id": 0,
      "question": "Which countries did James visit in July 2022?",
      "prediction": "Toronto and Vancouver",
      "reference": "Canada, Greenland",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8896384239196777,
        "bert_recall": 0.8896384239196777,
        "bert_f1": 0.8896384239196777,
        "meteor": 0.0,
        "sbert_similarity": 0.5298832654953003
      }
    },
    {
      "sample_id": 0,
      "question": "What additional country did James visit during his trip to Canada?",
      "prediction": "the city of Nuuk",
      "reference": "Greenland",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7977467775344849,
        "bert_recall": 0.873708963394165,
        "bert_f1": 0.8340017795562744,
        "meteor": 0.0,
        "sbert_similarity": 0.41717270016670227
      }
    },
    {
      "sample_id": 0,
      "question": "Who is Jill?",
      "prediction": "a personal relationship with the audience's companion",
      "reference": "Most likely John's partner.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.15384615384615385,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615385,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.874774694442749,
        "bert_recall": 0.8847848773002625,
        "bert_f1": 0.8797512650489807,
        "meteor": 0.0,
        "sbert_similarity": 0.30647751688957214
      }
    },
    {
      "sample_id": 0,
      "question": "When did John spend time with his sister and dogs?",
      "prediction": "22 July, 2022",
      "reference": "July 21, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.75,
        "bleu2": 0.49999999999999994,
        "bleu3": 0.23549424823785386,
        "bleu4": 0.1880301546543197,
        "bert_precision": 0.9338943362236023,
        "bert_recall": 0.9321179389953613,
        "bert_f1": 0.9330052733421326,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.9592170715332031
      }
    },
    {
      "sample_id": 0,
      "question": "What happened to John's job situation in 2022?",
      "prediction": "I finally got my dream job! After lots of interviews and late nights, I got the offer and was ecstatic.",
      "reference": "quit his IT Job, secured his dream job, aspires to become an eSports competition organizer",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.1142857142857143,
        "rouge2_f": 0.06060606060606061,
        "rougeL_f": 0.1142857142857143,
        "bleu1": 0.13043478260869565,
        "bleu2": 0.07699905035090181,
        "bleu3": 0.031531742858954555,
        "bleu4": 0.019383418023456653,
        "bert_precision": 0.8759079575538635,
        "bert_recall": 0.881205141544342,
        "bert_f1": 0.8785485625267029,
        "meteor": 0.03225806451612903,
        "sbert_similarity": 0.22621437907218933
      }
    },
    {
      "sample_id": 0,
      "question": "When did John start his job in IT?",
      "prediction": "next month",
      "reference": "2019",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8670518398284912,
        "bert_recall": 0.9255148768424988,
        "bert_f1": 0.8953299522399902,
        "meteor": 0.0,
        "sbert_similarity": 0.49602147936820984
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of tricks do James's pets know?",
      "prediction": "Max is awesome at catching frisbees in mid-air - never misses!",
      "reference": "swimming, catching frisbees, balancing on a skateboard, sit, stay, paw, and rollover",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.17391304347826086,
        "rouge1_f": 0.17391304347826086,
        "rouge2_f": 0.09523809523809525,
        "rougeL_f": 0.17391304347826086,
        "bleu1": 0.10108844328543891,
        "bleu2": 0.07465876322870636,
        "bleu3": 0.03329685848470086,
        "bleu4": 0.02184772133544285,
        "bert_precision": 0.8769156336784363,
        "bert_recall": 0.8498608469963074,
        "bert_f1": 0.8631762862205505,
        "meteor": 0.04201680672268908,
        "sbert_similarity": 0.3243844509124756
      }
    },
    {
      "sample_id": 0,
      "question": "When did James meet Samantha?",
      "prediction": "4 September, 2022",
      "reference": "August 9, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.5,
        "bleu2": 0.408248290463863,
        "bleu3": 0.20600140803015504,
        "bleu4": 0.16990442448471224,
        "bert_precision": 0.9480968713760376,
        "bert_recall": 0.9480968713760376,
        "bert_f1": 0.9480968713760376,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.7850077748298645
      }
    },
    {
      "sample_id": 0,
      "question": "When did James take his 3 dogs to the beach?",
      "prediction": "10 August, 2022",
      "reference": "August 9, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.75,
        "bleu2": 0.49999999999999994,
        "bleu3": 0.23549424823785386,
        "bleu4": 0.1880301546543197,
        "bert_precision": 0.9402245879173279,
        "bert_recall": 0.9376926422119141,
        "bert_f1": 0.9389569163322449,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.9265442490577698
      }
    },
    {
      "sample_id": 0,
      "question": "When did John plan his next meeting with his siblings?",
      "prediction": "next month after 21 August, 2022",
      "reference": "In September, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.2182178902359924,
        "bleu3": 0.1006949130683619,
        "bleu4": 0.06985342056580097,
        "bert_precision": 0.8745652437210083,
        "bert_recall": 0.9372477531433105,
        "bert_f1": 0.9048222303390503,
        "meteor": 0.15151515151515152,
        "sbert_similarity": 0.7461639046669006
      }
    },
    {
      "sample_id": 0,
      "question": "Why didn't John want to go to Starbucks?",
      "prediction": "I don't mind meeting up, but why Starbucks? Maybe we can have a beer somewhere?",
      "reference": "Possibly because he likes to drink beer on his days off.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.07692307692307691,
        "rouge1_f": 0.07407407407407407,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07407407407407407,
        "bleu1": 0.05263157894736841,
        "bleu2": 0.017099639201419235,
        "bleu3": 0.012523428510015045,
        "bleu4": 0.010182425646195498,
        "bert_precision": 0.8526216149330139,
        "bert_recall": 0.871605396270752,
        "bert_f1": 0.8620089888572693,
        "meteor": 0.043859649122807015,
        "sbert_similarity": 0.42428112030029297
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of beer does McGee's bar serve?",
      "prediction": "great stout",
      "reference": "Stout, lager",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.13562437855552414,
        "bleu3": 0.10556327625176448,
        "bleu4": 0.09069748827745895,
        "bert_precision": 0.8762835264205933,
        "bert_recall": 0.834973156452179,
        "bert_f1": 0.8551297187805176,
        "meteor": 0.0,
        "sbert_similarity": 0.7643132209777832
      }
    },
    {
      "sample_id": 0,
      "question": "When did John and James meet at McGee's bar?",
      "prediction": "tomorrow at McGee's Pub",
      "reference": "August 27, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8112051486968994,
        "bert_recall": 0.7982063889503479,
        "bert_f1": 0.8046533465385437,
        "meteor": 0.0,
        "sbert_similarity": 0.2660905718803406
      }
    },
    {
      "sample_id": 0,
      "question": "When did James ask Samantha to be his girlfriend?",
      "prediction": "yesterday, 3 September 2022",
      "reference": "September 3, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.8571428571428571,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.8,
        "bleu2": 0.1414213562373095,
        "bleu3": 0.08951377743388189,
        "bleu4": 0.07598356856515925,
        "bert_precision": 0.9204562902450562,
        "bert_recall": 0.9407246112823486,
        "bert_f1": 0.9304800629615784,
        "meteor": 0.32258064516129037,
        "sbert_similarity": 0.8848817348480225
      }
    },
    {
      "sample_id": 0,
      "question": "When did James, Samantha and John go to the baseball game together?",
      "prediction": "next Sunday after 4 September 2022",
      "reference": "September 11, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444444,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.08164965809277261,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.048549177170732344,
        "bert_precision": 0.8531413078308105,
        "bert_recall": 0.9002902507781982,
        "bert_f1": 0.8760818839073181,
        "meteor": 0.30303030303030304,
        "sbert_similarity": 0.6962041854858398
      }
    },
    {
      "sample_id": 0,
      "question": "What gaming equipments did John buy or refurbish?",
      "prediction": "new devices and refurbished my gaming desk",
      "reference": "Sennheiser headphones, Logitech mouse, gaming desk",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.30769230769230765,
        "rouge1_f": 0.30769230769230765,
        "rouge2_f": 0.1818181818181818,
        "rougeL_f": 0.30769230769230765,
        "bleu1": 0.24767939992862334,
        "bleu2": 0.18916826637569276,
        "bleu3": 0.08729019475622869,
        "bleu4": 0.0605543865104477,
        "bert_precision": 0.8686450719833374,
        "bert_recall": 0.8048352003097534,
        "bert_f1": 0.8355236053466797,
        "meteor": 0.3073770491803279,
        "sbert_similarity": 0.5282574892044067
      }
    },
    {
      "sample_id": 0,
      "question": "When did James start taking cooking classes?",
      "prediction": "two days ago, 2 September 2022",
      "reference": "September 2, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0.5714285714285714,
        "bleu2": 0.09759000729485333,
        "bleu3": 0.05920350650497893,
        "bleu4": 0.046713797772820004,
        "bert_precision": 0.8346714973449707,
        "bert_recall": 0.8939284086227417,
        "bert_f1": 0.8632842898368835,
        "meteor": 0.30303030303030304,
        "sbert_similarity": 0.829696774482727
      }
    },
    {
      "sample_id": 0,
      "question": "Which new games did John start play during the course of the conversation with James?",
      "prediction": "board games",
      "reference": "AC Valhalla, Witcher 3, FIFA 23, Dungeons of the Dragons, futuristic dystopian game",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.13333333333333336,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333336,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8966591358184814,
        "bert_recall": 0.8087400794029236,
        "bert_f1": 0.8504332900047302,
        "meteor": 0.04201680672268908,
        "sbert_similarity": 0.4839073419570923
      }
    },
    {
      "sample_id": 0,
      "question": "When did John start working on his 2D Adventure mobile game?",
      "prediction": "for the past few months before 20 September 2022",
      "reference": "approximately summer of 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615383,
        "rouge1_f": 0.15384615384615383,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615383,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.855770468711853,
        "bert_recall": 0.8717348575592041,
        "bert_f1": 0.8636788725852966,
        "meteor": 0.11111111111111112,
        "sbert_similarity": 0.7164250612258911
      }
    },
    {
      "sample_id": 0,
      "question": "How long did it take for James to complete his Witcher-inspired game?",
      "prediction": "It took several months since the last mention was on 13 October 2022.",
      "reference": "six months",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333336,
        "rouge1_f": 0.13333333333333336,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333336,
        "bleu1": 0.07142857142857141,
        "bleu2": 0.023440361546924773,
        "bleu3": 0.017300021018422027,
        "bleu4": 0.01428363257865929,
        "bert_precision": 0.8305891752243042,
        "bert_recall": 0.8835606575012207,
        "bert_f1": 0.8562564849853516,
        "meteor": 0.16129032258064518,
        "sbert_similarity": 0.4820899963378906
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of programming-related events has John hosted?",
      "prediction": "I organized the programming seminar last week.",
      "reference": "online programming competition, programming seminar",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.36363636363636365,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.2,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.25,
        "bleu2": 0.18898223650461357,
        "bleu3": 0.08622792534209149,
        "bleu4": 0.05873949094699214,
        "bert_precision": 0.9058737754821777,
        "bert_recall": 0.9174920320510864,
        "bert_f1": 0.9116458892822266,
        "meteor": 0.3605769230769231,
        "sbert_similarity": 0.629275918006897
      }
    },
    {
      "sample_id": 0,
      "question": "When did John and his programming friends host an online programming competition?",
      "prediction": "last week, 13 October 2022",
      "reference": "Last week before 13 October 2022.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.9090909090909091,
        "rouge1_f": 0.9090909090909091,
        "rouge2_f": 0.6666666666666665,
        "rougeL_f": 0.9090909090909091,
        "bleu1": 0.7054014374088451,
        "bleu2": 0.5985529678206387,
        "bleu3": 0.4261847354500941,
        "bleu4": 0.21506254256566312,
        "bert_precision": 0.9274861812591553,
        "bert_recall": 0.9329317808151245,
        "bert_f1": 0.93020099401474,
        "meteor": 0.43314500941619577,
        "sbert_similarity": 0.9437289834022522
      }
    },
    {
      "sample_id": 0,
      "question": "Which of James's family members have visited him in the last year?",
      "prediction": "my family",
      "reference": "mother, sister",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9065135717391968,
        "bert_recall": 0.871921181678772,
        "bert_f1": 0.8888809084892273,
        "meteor": 0.0,
        "sbert_similarity": 0.6260055899620056
      }
    },
    {
      "sample_id": 0,
      "question": "When did James' mother and her friend visit him?",
      "prediction": "not mentioned",
      "reference": "October 19, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8125589489936829,
        "bert_recall": 0.7967543601989746,
        "bert_f1": 0.8045790195465088,
        "meteor": 0.0,
        "sbert_similarity": 0.1433178186416626
      }
    },
    {
      "sample_id": 0,
      "question": "When did James try Cyberpunk 2077 game?",
      "prediction": "21 October, 2022",
      "reference": "October 20, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.75,
        "bleu2": 0.49999999999999994,
        "bleu3": 0.23549424823785386,
        "bleu4": 0.1880301546543197,
        "bert_precision": 0.9382932782173157,
        "bert_recall": 0.9382932782173157,
        "bert_f1": 0.9382932782173157,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.9551027417182922
      }
    },
    {
      "sample_id": 0,
      "question": "When did John and his gaming friends organize the charity tournament?",
      "prediction": "last night, 30 October 2022",
      "reference": "On the night of October 30 to 31, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285714,
        "rouge1_f": 0.5714285714285714,
        "rouge2_f": 0.0,
        "rougeL_f": 0.42857142857142855,
        "bleu1": 0.4278475991938267,
        "bleu2": 0.06628186505570068,
        "bleu3": 0.03935506514557255,
        "bleu4": 0.03134275885275486,
        "bert_precision": 0.9340420961380005,
        "bert_recall": 0.8987075090408325,
        "bert_f1": 0.9160341620445251,
        "meteor": 0.17441860465116277,
        "sbert_similarity": 0.8716824054718018
      }
    },
    {
      "sample_id": 0,
      "question": "What games has John played with his friends at charity tournaments?",
      "prediction": "Fortnite and a few other games.",
      "reference": "CS:GO, Fortnite, Overwatch and Apex Legends",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.30769230769230765,
        "rouge2_f": 0.0,
        "rougeL_f": 0.30769230769230765,
        "bleu1": 0.18612544500887301,
        "bleu2": 0.044953565761262394,
        "bleu3": 0.03068183462630887,
        "bleu4": 0.02558948059670209,
        "bert_precision": 0.8998235464096069,
        "bert_recall": 0.8524048328399658,
        "bert_f1": 0.8754726052284241,
        "meteor": 0.08333333333333333,
        "sbert_similarity": 0.6135333180427551
      }
    },
    {
      "sample_id": 0,
      "question": "What was James' big moment with Samantha in October 2023?",
      "prediction": "Samantha loves theater, I asked her to become my girlfriend, and she agreed.",
      "reference": "They decided to live together and rented an apartment not far from McGee's bar.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14814814814814817,
        "rouge1_f": 0.14285714285714288,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714288,
        "bleu1": 0.1875,
        "bleu2": 0.03535533905932738,
        "bleu3": 0.021565426544625413,
        "bleu4": 0.01618861356572822,
        "bert_precision": 0.8931424021720886,
        "bert_recall": 0.8798376321792603,
        "bert_f1": 0.8864401578903198,
        "meteor": 0.07194244604316546,
        "sbert_similarity": 0.10327745974063873
      }
    },
    {
      "sample_id": 0,
      "question": "How long did James and Samantha date for before deciding to move in together?",
      "prediction": "about a month",
      "reference": "nearly three months",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9408437013626099,
        "bert_recall": 0.9408437013626099,
        "bert_f1": 0.9408437013626099,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8118782043457031
      }
    },
    {
      "sample_id": 0,
      "question": "When did James, his family and his dogs start on a road trip together?",
      "prediction": "5 November, 2022",
      "reference": "November 4, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.75,
        "bleu2": 0.49999999999999994,
        "bleu3": 0.23549424823785386,
        "bleu4": 0.1880301546543197,
        "bert_precision": 0.93635094165802,
        "bert_recall": 0.9390464425086975,
        "bert_f1": 0.9376967549324036,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.9300607442855835
      }
    },
    {
      "sample_id": 0,
      "question": "How long did John practice chess for before winning the chess tournament?",
      "prediction": "about 6 months",
      "reference": "nearly four months",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9692919254302979,
        "bert_recall": 0.9692919254302979,
        "bert_f1": 0.9692919254302979,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.9149048924446106
      }
    },
    {
      "sample_id": 0,
      "question": "When did James and his family visit Mark and Josh?",
      "prediction": "on 7 November, 2022",
      "reference": "November 7, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.8571428571428571,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.8,
        "bleu2": 0.447213595499958,
        "bleu3": 0.19137706264177992,
        "bleu4": 0.13512001548070346,
        "bert_precision": 0.9338985085487366,
        "bert_recall": 0.9630386829376221,
        "bert_f1": 0.9482448101043701,
        "meteor": 0.16129032258064518,
        "sbert_similarity": 0.9439272284507751
      }
    },
    {
      "sample_id": 0,
      "question": "When did John work with a game developer on a project?",
      "prediction": "super fun weekend - 7 November 2022",
      "reference": "November 5-6, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.06900655593423542,
        "bleu3": 0.047098549390932394,
        "bleu4": 0.039281465090051315,
        "bert_precision": 0.8528217673301697,
        "bert_recall": 0.8523015975952148,
        "bert_f1": 0.8525615930557251,
        "meteor": 0.29411764705882354,
        "sbert_similarity": 0.5999393463134766
      }
    },
    {
      "sample_id": 0,
      "question": "What programming languages has James worked with?",
      "prediction": "I've worked with Python and C++.",
      "reference": "Python and C++",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6,
        "bleu1": 0.375,
        "bleu2": 0.32732683535398854,
        "bleu3": 0.2649097550589377,
        "bleu4": 0.1374708101760565,
        "bert_precision": 0.8926874399185181,
        "bert_recall": 0.9160831570625305,
        "bert_f1": 0.9042339324951172,
        "meteor": 0.5681818181818182,
        "sbert_similarity": 0.7837819457054138
      }
    },
    {
      "sample_id": 0,
      "question": "What type of mobile application does James plan to build with John?",
      "prediction": "an app for dog walking and pet care",
      "reference": "An app for dog walking and pet care",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9985811114311218,
        "bert_recall": 0.9985811114311218,
        "bert_f1": 0.9985811114311218,
        "meteor": 0.9990234375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "How does James plan to make his dog-sitting app unique?",
      "prediction": "The personal touch really sets it apart. Users can add their pup's preferences/needs - just like they were customizing it for them.",
      "reference": "By allowing users to customize their pup's preferences/needs",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.27586206896551724,
        "rouge1_f": 0.42424242424242425,
        "rouge2_f": 0.2580645161290322,
        "rougeL_f": 0.36363636363636365,
        "bleu1": 0.2,
        "bleu2": 0.158113883008419,
        "bleu3": 0.13221779664441874,
        "bleu4": 0.09970222994544457,
        "bert_precision": 0.8851827383041382,
        "bert_recall": 0.9324016571044922,
        "bert_f1": 0.9081788659095764,
        "meteor": 0.474468085106383,
        "sbert_similarity": 0.7879632115364075
      }
    },
    {
      "sample_id": 0,
      "question": "What has John mostly found with the metal detector so far?",
      "prediction": "Mostly just bottle caps",
      "reference": "bottle caps",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.408248290463863,
        "bleu3": 0.20600140803015504,
        "bleu4": 0.16990442448471224,
        "bert_precision": 0.864973783493042,
        "bert_recall": 0.8935117721557617,
        "bert_f1": 0.8790112137794495,
        "meteor": 0.8522727272727273,
        "sbert_similarity": 0.8349756002426147
      }
    },
    {
      "sample_id": 0,
      "question": "What did James offer to do for John regarding pets?",
      "prediction": "build an app for dog walking and pet care",
      "reference": "help find the perfect pet",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714285,
        "rouge1_f": 0.14285714285714285,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.8791428804397583,
        "bert_recall": 0.8813703060150146,
        "bert_f1": 0.8802551627159119,
        "meteor": 0.0925925925925926,
        "sbert_similarity": 0.4523515999317169
      }
    },
    {
      "sample_id": 0,
      "question": "What instrument is John learning to play as of 27 March, 2022?",
      "prediction": "drums",
      "reference": "Drums",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9235801100730896,
        "bert_recall": 0.7873409390449524,
        "bert_f1": 0.8500362038612366,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "How long has John been playing the drums as of 27 March, 2022?",
      "prediction": "I used to play drums when I was younger.",
      "reference": "One month",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8194100856781006,
        "bert_recall": 0.8391066789627075,
        "bert_f1": 0.8291414380073547,
        "meteor": 0.1851851851851852,
        "sbert_similarity": 0.1140822172164917
      }
    },
    {
      "sample_id": 0,
      "question": "What game did John play in an intense tournament at the gaming convention in March 2022?",
      "prediction": "CS:GO",
      "reference": "CS:GO",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What game was James playing in the online gaming tournament in April 2022?",
      "prediction": "the online gaming tournament",
      "reference": "Apex Legends",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8551582098007202,
        "bert_recall": 0.8645150661468506,
        "bert_f1": 0.8598111867904663,
        "meteor": 0.0,
        "sbert_similarity": 0.3820822834968567
      }
    },
    {
      "sample_id": 0,
      "question": "How does James communicate with his gaming team?",
      "prediction": "you always need to communicate correctly with the team",
      "reference": "voice chat",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8434897065162659,
        "bert_recall": 0.8698492050170898,
        "bert_f1": 0.8564667105674744,
        "meteor": 0.0,
        "sbert_similarity": 0.30018019676208496
      }
    },
    {
      "sample_id": 0,
      "question": "What advice did James receive from the famous players he met at the tournament?",
      "prediction": "I tried my best but didn't make it. It was close, though, and I had a blast competing with talented players.",
      "reference": "never put your ego above team success",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8593805432319641,
        "bert_recall": 0.8500267267227173,
        "bert_f1": 0.8546779751777649,
        "meteor": 0.0,
        "sbert_similarity": 0.4024219810962677
      }
    },
    {
      "sample_id": 0,
      "question": "What did James adopt in April 2022?",
      "prediction": "a pup",
      "reference": "a pup",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.000000238418579,
        "bert_recall": 1.000000238418579,
        "bert_f1": 1.000000238418579,
        "meteor": 0.9375,
        "sbert_similarity": 0.9999998807907104
      }
    },
    {
      "sample_id": 0,
      "question": "What is the name of the pup that was adopted by James?",
      "prediction": "Ned",
      "reference": "Ned",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "Why did James embody the appearance of the game character from the woman he saw during a walk?",
      "prediction": "It seemed to me that I fell in love at first sight.",
      "reference": "He found her appearance and eyes amazing.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.025318484177091663,
        "bleu3": 0.018733084686676876,
        "bleu4": 0.01553712569276035,
        "bert_precision": 0.8676201105117798,
        "bert_recall": 0.8897950053215027,
        "bert_f1": 0.8785676956176758,
        "meteor": 0.06666666666666667,
        "sbert_similarity": 0.35556143522262573
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired James to create the game character in the virtual world?",
      "prediction": "I'm working on something I've wanted to do since I was a kid. Even as a child, I made some sketches of the main character.",
      "reference": "Appearance of a woman he saw during a walk",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.12903225806451613,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 0.10000000000000002,
        "bleu2": 0.018569533817705194,
        "bleu3": 0.011216219772634475,
        "bleu4": 0.00821807407726565,
        "bert_precision": 0.844936728477478,
        "bert_recall": 0.8564275503158569,
        "bert_f1": 0.8506433367729187,
        "meteor": 0.14150943396226415,
        "sbert_similarity": 0.06258067488670349
      }
    },
    {
      "sample_id": 0,
      "question": "Which country did James visit in 2021?",
      "prediction": "Italy",
      "reference": "Italy",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What impresses John about Japan?",
      "prediction": "the megacities of this country impress me, everything there is so technologically advanced, the huge screens on the buildings are mesmerizing",
      "reference": "Technologically advanced megacities and tasty street food",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.23076923076923078,
        "rouge1_f": 0.21428571428571427,
        "rouge2_f": 0.07692307692307691,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.13043478260869565,
        "bleu2": 0.07699905035090181,
        "bleu3": 0.031531742858954555,
        "bleu4": 0.019383418023456653,
        "bert_precision": 0.8587258458137512,
        "bert_recall": 0.8833579421043396,
        "bert_f1": 0.8708677291870117,
        "meteor": 0.11904761904761903,
        "sbert_similarity": 0.4243939518928528
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of assignment was giving John a hard time at work?",
      "prediction": "a difficult assignment that involves coding",
      "reference": "Coding assignment",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.08164965809277261,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.048549177170732344,
        "bert_precision": 0.8661010265350342,
        "bert_recall": 0.9085683822631836,
        "bert_f1": 0.8868266344070435,
        "meteor": 0.4166666666666667,
        "sbert_similarity": 0.8613947033882141
      }
    },
    {
      "sample_id": 0,
      "question": "What breed is Daisy, one of James' dogs?",
      "prediction": "a Labrador",
      "reference": "Labrador",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.8626532554626465,
        "bert_recall": 0.9453296065330505,
        "bert_f1": 0.9021010994911194,
        "meteor": 0.45454545454545453,
        "sbert_similarity": 0.9615089893341064
      }
    },
    {
      "sample_id": 0,
      "question": "What type of pizza is James' favorite?",
      "prediction": "cheese pizza and prosciutto",
      "reference": "Pepperoni",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8068689107894897,
        "bert_recall": 0.819864809513092,
        "bert_f1": 0.8133149147033691,
        "meteor": 0.0,
        "sbert_similarity": 0.5360560417175293
      }
    },
    {
      "sample_id": 0,
      "question": "What type of pizza is John's favorite?",
      "prediction": "Hawaiian pizza",
      "reference": "Hawaiian",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.8122694492340088,
        "bert_recall": 0.8573688268661499,
        "bert_f1": 0.8342100977897644,
        "meteor": 0.45454545454545453,
        "sbert_similarity": 0.8069418668746948
      }
    },
    {
      "sample_id": 0,
      "question": "What did John organize with his friends on May 8, 2022?",
      "prediction": "more events like that",
      "reference": "A tournament for CS:GO",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8653500080108643,
        "bert_recall": 0.8232659101486206,
        "bert_f1": 0.8437834978103638,
        "meteor": 0.0,
        "sbert_similarity": 0.281310111284256
      }
    },
    {
      "sample_id": 0,
      "question": "What did John and his friends do with the remaining money after helping the dog shelter?",
      "prediction": "use this money to buy groceries and cook some food for the homeless.",
      "reference": "Bought groceries and cooked food for the homeless",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285714,
        "rouge1_f": 0.6666666666666667,
        "rouge2_f": 0.5263157894736842,
        "rougeL_f": 0.6666666666666667,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.36313651960128146,
        "bleu3": 0.2836979771131725,
        "bleu4": 0.21142141714303078,
        "bert_precision": 0.9108008146286011,
        "bert_recall": 0.9367085695266724,
        "bert_f1": 0.9235730767250061,
        "meteor": 0.6928104575163399,
        "sbert_similarity": 0.674324095249176
      }
    },
    {
      "sample_id": 0,
      "question": "What was the main goal of the money raised from the charity tournament organized by John and his friends in May 2022?",
      "prediction": "made a bunch of money for charity",
      "reference": "Raise money for a dog shelter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4615384615384615,
        "rouge1_f": 0.4615384615384615,
        "rouge2_f": 0.1818181818181818,
        "rougeL_f": 0.30769230769230765,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.26726124191242434,
        "bleu3": 0.11511121735118796,
        "bleu4": 0.07730551756939454,
        "bert_precision": 0.8946072459220886,
        "bert_recall": 0.9006859064102173,
        "bert_f1": 0.8976362943649292,
        "meteor": 0.4189435336976321,
        "sbert_similarity": 0.4750741422176361
      }
    },
    {
      "sample_id": 0,
      "question": "What did the system John created help the charitable foundation with?",
      "prediction": "streamline their operations and make them run more smoothly",
      "reference": "tracking inventory, resources, and donations",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714285,
        "rouge1_f": 0.14285714285714285,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.82542484998703,
        "bert_recall": 0.87070631980896,
        "bert_f1": 0.8474611043930054,
        "meteor": 0.0925925925925926,
        "sbert_similarity": 0.06101603806018829
      }
    },
    {
      "sample_id": 0,
      "question": "What did John create for the charitable foundation that helped generate reports for analysis?",
      "prediction": "a software tool",
      "reference": "computer application on smartphones",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.909342348575592,
        "bert_recall": 0.8818779587745667,
        "bert_f1": 0.8953996300697327,
        "meteor": 0.0,
        "sbert_similarity": 0.2657947540283203
      }
    },
    {
      "sample_id": 0,
      "question": "What did John receive for achieving second place in the tournament?",
      "prediction": "some money for the 2nd place",
      "reference": "money and a trophy",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.8463928699493408,
        "bert_recall": 0.8470379114151001,
        "bert_f1": 0.8467152714729309,
        "meteor": 0.11904761904761905,
        "sbert_similarity": 0.6002836227416992
      }
    },
    {
      "sample_id": 0,
      "question": "What project is James working on in his game design course?",
      "prediction": "I want to turn it into a computer game.",
      "reference": "a new part of the football simulator, collecting player databases",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10526315789473685,
        "rouge1_f": 0.10526315789473685,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473685,
        "bleu1": 0.09048374180359597,
        "bleu2": 0.03016124726786533,
        "bleu3": 0.022576199178367274,
        "bleu4": 0.01909690192411307,
        "bert_precision": 0.8846408128738403,
        "bert_recall": 0.8722091913223267,
        "bert_f1": 0.8783810138702393,
        "meteor": 0.05050505050505051,
        "sbert_similarity": 0.3156377971172333
      }
    },
    {
      "sample_id": 0,
      "question": "Who does James support in football matches?",
      "prediction": "Liverpool",
      "reference": "Liverpool",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "Which football club does John support?",
      "prediction": "Manchester City",
      "reference": "Manchester City",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What disagreement do James and John have about their football teams?",
      "prediction": "I'm sure you're wrong, John. Manchester City are in bad form and their transfer policy is terrible!",
      "reference": "debating on which team will perform better in the championship",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.07407407407407408,
        "rouge1_f": 0.06896551724137931,
        "rouge2_f": 0.0,
        "rougeL_f": 0.06896551724137931,
        "bleu1": 0.04545454545454546,
        "bleu2": 0.014712247158412498,
        "bleu3": 0.010748014644407517,
        "bleu4": 0.008687475782716616,
        "bert_precision": 0.8449388742446899,
        "bert_recall": 0.8608500361442566,
        "bert_f1": 0.8528202772140503,
        "meteor": 0.04672897196261682,
        "sbert_similarity": 0.17229557037353516
      }
    },
    {
      "sample_id": 0,
      "question": "What is Max good at doing according to James?",
      "prediction": "He's a pro swimmer!",
      "reference": "catching frisbees in mid-air",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8417543768882751,
        "bert_recall": 0.8209420442581177,
        "bert_f1": 0.8312180042266846,
        "meteor": 0.0,
        "sbert_similarity": 0.1313483566045761
      }
    },
    {
      "sample_id": 0,
      "question": "What is the main focus of the organization that James volunteered with?",
      "prediction": "provides necessary items to those who are less fortunate",
      "reference": "providing necessary items to those who are less fortunate",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8888888888888888,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.8888888888888888,
        "bleu2": 0.8819171036881969,
        "bleu3": 0.8747619509674373,
        "bleu4": 0.8633400213704505,
        "bert_precision": 0.9999054670333862,
        "bert_recall": 0.9999054670333862,
        "bert_f1": 0.9999054670333862,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 0.9650890827178955
      }
    },
    {
      "sample_id": 0,
      "question": "Will there be an interview required to volunteer with the organization James volunteered for?",
      "prediction": "The content does not mention an interview required to volunteer.",
      "reference": "No",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8039306998252869,
        "bert_recall": 0.8257702589035034,
        "bert_f1": 0.814704179763794,
        "meteor": 0.0,
        "sbert_similarity": 0.04361134022474289
      }
    },
    {
      "sample_id": 0,
      "question": "How did John relax in his free time on 9 July, 2022?",
      "prediction": "I had a few new friends over to watch some movies.",
      "reference": "Reading",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8229869604110718,
        "bert_recall": 0.8542625904083252,
        "bert_f1": 0.8383331894874573,
        "meteor": 0.0,
        "sbert_similarity": 0.20243379473686218
      }
    },
    {
      "sample_id": 0,
      "question": "What did James enjoy doing on cold winter days?",
      "prediction": "I also love to read, especially while snuggled under the covers on a cold winter day.",
      "reference": "Reading while snuggled under the covers",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.45454545454545453,
        "rouge1_f": 0.5454545454545454,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5454545454545454,
        "bleu1": 0.2777777777777778,
        "bleu2": 0.25565499628245686,
        "bleu3": 0.23396034203975458,
        "bleu4": 0.20105373454060027,
        "bert_precision": 0.8911203145980835,
        "bert_recall": 0.9478653073310852,
        "bert_f1": 0.9186172485351562,
        "meteor": 0.7114285714285715,
        "sbert_similarity": 0.7413955926895142
      }
    },
    {
      "sample_id": 0,
      "question": "What new hobby did James become interested in on 9 July, 2022?",
      "prediction": "Lately, I've been checking out different styles of it.",
      "reference": "Extreme sports",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8329812288284302,
        "bert_recall": 0.803993821144104,
        "bert_f1": 0.8182308673858643,
        "meteor": 0.0,
        "sbert_similarity": 0.06646531075239182
      }
    },
    {
      "sample_id": 0,
      "question": "Where did James plan to visit after Toronto?",
      "prediction": "I also plan to visit Vancouver.",
      "reference": "Vancouver",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2857142857142857,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8434247970581055,
        "bert_recall": 0.8515750169754028,
        "bert_f1": 0.847480297088623,
        "meteor": 0.0,
        "sbert_similarity": 0.6568424701690674
      }
    },
    {
      "sample_id": 0,
      "question": "When did James plan to return from his trip to Toronto and Vancouver?",
      "prediction": "I plan to return on July 20",
      "reference": "July 20",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444445,
        "rouge1_f": 0.4444444444444445,
        "rouge2_f": 0.2857142857142857,
        "rougeL_f": 0.4444444444444445,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.2182178902359924,
        "bleu3": 0.1006949130683619,
        "bleu4": 0.06985342056580097,
        "bert_precision": 0.8274290561676025,
        "bert_recall": 0.8814456462860107,
        "bert_f1": 0.8535836338996887,
        "meteor": 0.75,
        "sbert_similarity": 0.5501654744148254
      }
    },
    {
      "sample_id": 0,
      "question": "What online game did John start playing recently for improving strategy?",
      "prediction": "AC Valhalla",
      "reference": "Chess",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8644587993621826,
        "bert_recall": 0.927353024482727,
        "bert_f1": 0.8948020935058594,
        "meteor": 0.0,
        "sbert_similarity": 0.209699347615242
      }
    },
    {
      "sample_id": 0,
      "question": "What made John leave his IT job?",
      "prediction": "I wanted something that made a difference.",
      "reference": "to focus on things that align with his values and passions",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1111111111111111,
        "rouge1_f": 0.1111111111111111,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 0.08591115984887154,
        "bleu2": 0.02904327290997419,
        "bleu3": 0.02205198090497798,
        "bleu4": 0.019090277826420414,
        "bert_precision": 0.9049643278121948,
        "bert_recall": 0.8711985349655151,
        "bert_f1": 0.8877604603767395,
        "meteor": 0.04716981132075472,
        "sbert_similarity": 0.21025097370147705
      }
    },
    {
      "sample_id": 0,
      "question": "Which game tournaments does John plan to organize besides CS:GO?",
      "prediction": "Fortnite competitions.",
      "reference": "Fortnite competitions",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.24028114141347542,
        "bert_precision": 0.991850733757019,
        "bert_recall": 0.9898211359977722,
        "bert_f1": 0.9908349514007568,
        "meteor": 0.25,
        "sbert_similarity": 0.9751707315444946
      }
    },
    {
      "sample_id": 0,
      "question": "What happened to James's puppy during the recent visit to the clinic?",
      "prediction": "This was just a routine examination. Also, the puppy was vaccinated to prevent him from catching the seasonal canine disease.",
      "reference": "routine examination and vaccination",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1818181818181818,
        "rouge1_f": 0.24999999999999997,
        "rouge2_f": 0.09090909090909091,
        "rougeL_f": 0.24999999999999997,
        "bleu1": 0.08695652173913045,
        "bleu2": 0.06286946134619315,
        "bleu3": 0.02758276890070265,
        "bleu4": 0.017514895362802613,
        "bert_precision": 0.8430001139640808,
        "bert_recall": 0.8764205574989319,
        "bert_f1": 0.8593855500221252,
        "meteor": 0.17857142857142858,
        "sbert_similarity": 0.49289414286613464
      }
    },
    {
      "sample_id": 0,
      "question": "What game genre did John start exploring instead of shooters?",
      "prediction": "strategy and RPG games",
      "reference": "strategy and RPG games",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "Which RPG game is John playing and enjoying on 10 August, 2022?",
      "prediction": "a new RPG that I've been playing lately",
      "reference": "The Witcher 3",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8300879001617432,
        "bert_recall": 0.8632226586341858,
        "bert_f1": 0.8463311195373535,
        "meteor": 0.0,
        "sbert_similarity": 0.4701690673828125
      }
    },
    {
      "sample_id": 0,
      "question": "What aspect of \"The Witcher 3\" does John find immersive?",
      "prediction": "the player's involvement in decision-making that impacts the game world",
      "reference": "shaping the world with choices",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2857142857142857,
        "rouge1_f": 0.23529411764705882,
        "rouge2_f": 0.0,
        "rougeL_f": 0.23529411764705882,
        "bleu1": 0.18181818181818182,
        "bleu2": 0.042640143271122095,
        "bleu3": 0.028234511901075607,
        "bleu4": 0.022416933501922302,
        "bert_precision": 0.8421151041984558,
        "bert_recall": 0.8591672778129578,
        "bert_f1": 0.8505557179450989,
        "meteor": 0.18181818181818182,
        "sbert_similarity": 0.48765096068382263
      }
    },
    {
      "sample_id": 0,
      "question": "Whose phone number did James receive during the beach outing?",
      "prediction": "a beautiful girl",
      "reference": "Samantha",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8282580375671387,
        "bert_recall": 0.8814123868942261,
        "bert_f1": 0.8540089130401611,
        "meteor": 0.0,
        "sbert_similarity": 0.38208624720573425
      }
    },
    {
      "sample_id": 0,
      "question": "What is James planning to do after receiving Samantha's phone number?",
      "prediction": "I'll definitely call her!",
      "reference": "call her",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.25819888974716115,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.08633400213704504,
        "bert_precision": 0.818709671497345,
        "bert_recall": 0.8828945159912109,
        "bert_f1": 0.8495916128158569,
        "meteor": 0.22727272727272727,
        "sbert_similarity": 0.6811602711677551
      }
    },
    {
      "sample_id": 0,
      "question": "What is John organizing with his siblings?",
      "prediction": "one with my siblings next month",
      "reference": "a gaming night",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8465365171432495,
        "bert_recall": 0.8595961928367615,
        "bert_f1": 0.8530163764953613,
        "meteor": 0.0,
        "sbert_similarity": 0.2569260001182556
      }
    },
    {
      "sample_id": 0,
      "question": "What type of beer does John not like?",
      "prediction": "dark beer",
      "reference": "dark beer",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What were some difficulties James faced during the development of his game?",
      "prediction": "What do you think is the most difficult thing about this game?",
      "reference": "balancing mechanics and ensuring fairness",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.82180255651474,
        "bert_recall": 0.8351500034332275,
        "bert_f1": 0.828422486782074,
        "meteor": 0.0,
        "sbert_similarity": 0.19882044196128845
      }
    },
    {
      "sample_id": 0,
      "question": "What has John been teaching his siblings?",
      "prediction": "coding",
      "reference": "coding",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of programs are John's siblings making?",
      "prediction": "they're already creating their own programs - amazing!",
      "reference": "basic games and stories",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.843568742275238,
        "bert_recall": 0.8705554604530334,
        "bert_f1": 0.8568496704101562,
        "meteor": 0.0,
        "sbert_similarity": 0.21624383330345154
      }
    },
    {
      "sample_id": 0,
      "question": "Which company's headphones did John choose for gaming?",
      "prediction": "headphones from Sennheiser",
      "reference": "Sennheiser",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.8704642057418823,
        "bert_recall": 0.8760042190551758,
        "bert_f1": 0.8732254505157471,
        "meteor": 0.4166666666666667,
        "sbert_similarity": 0.769008994102478
      }
    },
    {
      "sample_id": 0,
      "question": "What did James and Samantha discover they both enjoy at McGee's bar?",
      "prediction": "a good lager beer",
      "reference": "Lager beer",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.408248290463863,
        "bleu3": 0.20600140803015504,
        "bleu4": 0.16990442448471224,
        "bert_precision": 0.8953527212142944,
        "bert_recall": 0.9286588430404663,
        "bert_f1": 0.9117016792297363,
        "meteor": 0.8522727272727273,
        "sbert_similarity": 0.8758431077003479
      }
    },
    {
      "sample_id": 0,
      "question": "How much does James pay per cooking class?",
      "prediction": "$10 per class",
      "reference": "$10",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.5,
        "bleu2": 0.408248290463863,
        "bleu3": 0.20600140803015504,
        "bleu4": 0.16990442448471224,
        "bert_precision": 0.8627243041992188,
        "bert_recall": 0.9250651597976685,
        "bert_f1": 0.8928078413009644,
        "meteor": 0.4166666666666667,
        "sbert_similarity": 0.5239687561988831
      }
    },
    {
      "sample_id": 0,
      "question": "What did James learn to make in the cooking class besides omelette and meringue?",
      "prediction": "dough",
      "reference": "Dough",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9990586638450623,
        "bert_recall": 0.9990586638450623,
        "bert_f1": 0.9990586638450623,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "Why did James sign up for a cooking class?",
      "prediction": "I felt that I wanted to learn something new.",
      "reference": "He wanted to learn something new",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7142857142857143,
        "rouge1_f": 0.6666666666666667,
        "rouge2_f": 0.6153846153846154,
        "rougeL_f": 0.6666666666666667,
        "bleu1": 0.5,
        "bleu2": 0.4714045207910317,
        "bleu3": 0.44042320075255215,
        "bleu4": 0.392814650900513,
        "bert_precision": 0.9552435874938965,
        "bert_recall": 0.9747605323791504,
        "bert_f1": 0.9649033546447754,
        "meteor": 0.6299603174603176,
        "sbert_similarity": 0.6503679156303406
      }
    },
    {
      "sample_id": 0,
      "question": "What did James prepare for the first time in the cooking class?",
      "prediction": "I got a great omelette the first time!",
      "reference": "Omelette",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.8460711240768433,
        "bert_recall": 0.8802198171615601,
        "bert_f1": 0.8628076910972595,
        "meteor": 0.2941176470588236,
        "sbert_similarity": 0.7402145266532898
      }
    },
    {
      "sample_id": 0,
      "question": "What is the name of the board game John tried in September 2022?",
      "prediction": "the board game",
      "reference": "Dungeons of the Dragon",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.23884377019126307,
        "bleu2": 0.09250379442961536,
        "bleu3": 0.08678511835352629,
        "bleu4": 0.0814136751754278,
        "bert_precision": 0.8366462588310242,
        "bert_recall": 0.82184898853302,
        "bert_f1": 0.8291816115379333,
        "meteor": 0.12820512820512822,
        "sbert_similarity": 0.41894686222076416
      }
    },
    {
      "sample_id": 0,
      "question": "Where does James get his ideas from?",
      "prediction": "various sources like books, movies, and even dreams.",
      "reference": "books, movies, dreams",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.5454545454545454,
        "rouge2_f": 0.22222222222222224,
        "rougeL_f": 0.5454545454545454,
        "bleu1": 0.45454545454545453,
        "bleu2": 0.3692744729379982,
        "bleu3": 0.3154208515212155,
        "bleu4": 0.24808415001701817,
        "bert_precision": 0.8604239821434021,
        "bert_recall": 0.8516100645065308,
        "bert_f1": 0.8559943437576294,
        "meteor": 0.5357142857142858,
        "sbert_similarity": 0.6834322214126587
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of dream did James have recently?",
      "prediction": "a crazy dream that led to some creative ideas",
      "reference": "a dream with a medieval castle full of puzzles and traps",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2105263157894737,
        "rouge1_f": 0.19999999999999998,
        "rouge2_f": 0.0,
        "rougeL_f": 0.19999999999999998,
        "bleu1": 0.17794164509262397,
        "bleu2": 0.04220256668175114,
        "bleu3": 0.02825061695601845,
        "bleu4": 0.022835060540062107,
        "bert_precision": 0.887432873249054,
        "bert_recall": 0.8859707117080688,
        "bert_f1": 0.8867011070251465,
        "meteor": 0.09259259259259259,
        "sbert_similarity": 0.5527693033218384
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of music does John like?",
      "prediction": "Definitely! Trying new genres is always exciting.",
      "reference": "electronic and rock music",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8439846038818359,
        "bert_recall": 0.828048825263977,
        "bert_f1": 0.8359407782554626,
        "meteor": 0.0,
        "sbert_similarity": 0.3818521499633789
      }
    },
    {
      "sample_id": 0,
      "question": "What instrument did James used to play when he was younger?",
      "prediction": "drums",
      "reference": "guitar",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9944282174110413,
        "bert_recall": 0.9944282174110413,
        "bert_f1": 0.9944282174110413,
        "meteor": 0.0,
        "sbert_similarity": 0.529847264289856
      }
    },
    {
      "sample_id": 0,
      "question": "What did John use to play when he was younger to let off steam?",
      "prediction": "Gaming is great for chilling out.",
      "reference": "drums",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8365947008132935,
        "bert_recall": 0.8703282475471497,
        "bert_f1": 0.8531280755996704,
        "meteor": 0.0,
        "sbert_similarity": 0.0808122456073761
      }
    },
    {
      "sample_id": 0,
      "question": "What career milestone did John achieve recently in September 2022?",
      "prediction": "making my first mobile game",
      "reference": "making his first mobile game",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8000000000000002,
        "rouge1_f": 0.8000000000000002,
        "rouge2_f": 0.5,
        "rougeL_f": 0.8000000000000002,
        "bleu1": 0.8,
        "bleu2": 0.6324555320336759,
        "bleu3": 0.5143157015215767,
        "bleu4": 0.28574404296988,
        "bert_precision": 0.9847745895385742,
        "bert_recall": 0.9847745895385742,
        "bert_f1": 0.9847745895385742,
        "meteor": 0.7500000000000001,
        "sbert_similarity": 0.8308532238006592
      }
    },
    {
      "sample_id": 0,
      "question": "What type of game is John's upcoming mobile game?",
      "prediction": "mobile game",
      "reference": "2D adventure",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8625824451446533,
        "bert_recall": 0.8094418048858643,
        "bert_f1": 0.835167646408081,
        "meteor": 0.0,
        "sbert_similarity": 0.43236044049263
      }
    },
    {
      "sample_id": 0,
      "question": "What does John do to stay informed and constantly learn about game design?",
      "prediction": "watch tutorials and keep up with developer forums for information and ideas",
      "reference": "watch tutorials and keep up with developer forums",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8421052631578948,
        "rouge1_f": 0.8,
        "rouge2_f": 0.7777777777777778,
        "rougeL_f": 0.8,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.6513389472789296,
        "bleu3": 0.6366526692301232,
        "bleu4": 0.6132297420585351,
        "bert_precision": 0.9412994384765625,
        "bert_recall": 0.9886502027511597,
        "bert_f1": 0.9643939733505249,
        "meteor": 0.9272693452380953,
        "sbert_similarity": 0.9015539884567261
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of gig was John offered at the game dev non-profit organization?",
      "prediction": "a volunteer gig at a game dev non-profit",
      "reference": "programming mentor for game developers",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.14285714285714285,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.8872722387313843,
        "bert_recall": 0.9104955792427063,
        "bert_f1": 0.898733913898468,
        "meteor": 0.09433962264150944,
        "sbert_similarity": 0.4303768277168274
      }
    },
    {
      "sample_id": 0,
      "question": "What does John feel about starting the journey as a programming mentor for game developers?",
      "prediction": "I'm really excited to share my knowledge and motivate people who are passionate about gaming.",
      "reference": "excited and inspired",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.21052631578947367,
        "rouge2_f": 0.0,
        "rougeL_f": 0.21052631578947367,
        "bleu1": 0.11764705882352941,
        "bleu2": 0.027116307227332027,
        "bleu3": 0.017693837921724947,
        "bleu4": 0.013679192123121896,
        "bert_precision": 0.8396202921867371,
        "bert_recall": 0.8588882684707642,
        "bert_f1": 0.8491450548171997,
        "meteor": 0.23809523809523808,
        "sbert_similarity": 0.41469186544418335
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of games is James excited to play with his new video card?",
      "prediction": "I'm getting into different types of games now, like RPGs and strategy games. It's really exciting!",
      "reference": "RPGs",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.10526315789473684,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473684,
        "bleu1": 0.047619047619047616,
        "bleu2": 0.01543033499620919,
        "bleu3": 0.011280777824436245,
        "bleu4": 0.009134423666564471,
        "bert_precision": 0.8081803321838379,
        "bert_recall": 0.8211068511009216,
        "bert_f1": 0.8145923018455505,
        "meteor": 0.20000000000000004,
        "sbert_similarity": 0.6223062872886658
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired James to create his game?",
      "prediction": "Witcher 3 gave me a ton of inspiration, with its amazing world and story.",
      "reference": "Witcher 3",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.14285714285714288,
        "rougeL_f": 0.25,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.04033183084278514,
        "bleu4": 0.026012784404037925,
        "bert_precision": 0.8370333313941956,
        "bert_recall": 0.8791239261627197,
        "bert_f1": 0.8575625419616699,
        "meteor": 0.5859375000000001,
        "sbert_similarity": 0.7260963916778564
      }
    },
    {
      "sample_id": 0,
      "question": "What sparked James' passion for gaming when he was a kid?",
      "prediction": "It was my first gaming system and I'd play Super Mario and The Legend of Zelda for hours.",
      "reference": "Super Mario and The Legend of Zelda games",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.56,
        "rouge1_f": 0.5925925925925926,
        "rouge2_f": 0.48,
        "rougeL_f": 0.5185185185185185,
        "bleu1": 0.3499999999999999,
        "bleu2": 0.3324549831021843,
        "bleu3": 0.31678444888358886,
        "bleu4": 0.2915369229944523,
        "bert_precision": 0.8770148754119873,
        "bert_recall": 0.922128438949585,
        "bert_f1": 0.8990060091018677,
        "meteor": 0.8819444444444445,
        "sbert_similarity": 0.6971727609634399
      }
    },
    {
      "sample_id": 0,
      "question": "What did James lose progress on due to a power outage?",
      "prediction": "some of it because I forgot to save",
      "reference": "a game",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8203169703483582,
        "bert_recall": 0.8703476190567017,
        "bert_f1": 0.8445920944213867,
        "meteor": 0.0,
        "sbert_similarity": 0.1565970480442047
      }
    },
    {
      "sample_id": 0,
      "question": "What games were played at the gaming tournament organized by John on 31 October, 2022?",
      "prediction": "Fortnite and a few other games",
      "reference": "Fortnite, Overwatch, Apex Legends",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.922764778137207,
        "bert_recall": 0.900941789150238,
        "bert_f1": 0.911722719669342,
        "meteor": 0.0,
        "sbert_similarity": 0.7047827839851379
      }
    },
    {
      "sample_id": 0,
      "question": "What was the purpose of the gaming tournament organized by John on 31 October, 2022?",
      "prediction": "fundraising for a children's hospital",
      "reference": "To raise money for a children's hospital",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.7142857142857143,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7142857142857143,
        "bleu1": 0.5971094254781577,
        "bleu2": 0.5850453652111616,
        "bleu3": 0.5700267993141759,
        "bleu4": 0.5444460596606694,
        "bert_precision": 0.9505955576896667,
        "bert_recall": 0.9149634838104248,
        "bert_f1": 0.9324392080307007,
        "meteor": 0.5836397058823528,
        "sbert_similarity": 0.8625031113624573
      }
    },
    {
      "sample_id": 0,
      "question": "What decision did James and Samantha make on 31 October, 2022?",
      "prediction": "Samantha and I have decided to move in together!",
      "reference": "To move in together",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6153846153846153,
        "rouge1_f": 0.6153846153846153,
        "rouge2_f": 0.5454545454545454,
        "rougeL_f": 0.6153846153846153,
        "bleu1": 0.4,
        "bleu2": 0.36514837167011077,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.2626909894424158,
        "bert_precision": 0.8870671987533569,
        "bert_recall": 0.9384961128234863,
        "bert_f1": 0.9120572209358215,
        "meteor": 0.654320987654321,
        "sbert_similarity": 0.4622298777103424
      }
    },
    {
      "sample_id": 0,
      "question": "Where did James and Samantha decide to live together on 31 October, 2022?",
      "prediction": "James and Samantha decided to move in together.",
      "reference": "In an apartment not far from McGee's bar",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.11764705882352941,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352941,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.8739291429519653,
        "bert_recall": 0.8449170589447021,
        "bert_f1": 0.8591783046722412,
        "meteor": 0.0625,
        "sbert_similarity": 0.1807488650083542
      }
    },
    {
      "sample_id": 0,
      "question": "Why did James and Samantha choose an apartment near McGee's bar?",
      "prediction": "one of the criteria for our choice of apartment was this particular bar nearby.",
      "reference": "They love spending time together at the bar",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1904761904761905,
        "rouge1_f": 0.18181818181818182,
        "rouge2_f": 0.0,
        "rougeL_f": 0.18181818181818182,
        "bleu1": 0.13333333333333333,
        "bleu2": 0.030860669992418384,
        "bleu3": 0.020202551616772847,
        "bleu4": 0.015718877363021206,
        "bert_precision": 0.8642392158508301,
        "bert_recall": 0.8828566074371338,
        "bert_f1": 0.8734487295150757,
        "meteor": 0.11627906976744187,
        "sbert_similarity": 0.3166431486606598
      }
    },
    {
      "sample_id": 0,
      "question": "What game is John hooked on playing on 5 November, 2022?",
      "prediction": "FIFA 23",
      "reference": "FIFA 23",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9375,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 0,
      "question": "What did John suggest James practice before playing FIFA 23 together?",
      "prediction": "You need to practice a little first",
      "reference": "Control with a gamepad and timing",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615383,
        "rouge1_f": 0.15384615384615383,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615383,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8561155200004578,
        "bert_recall": 0.8468881845474243,
        "bert_f1": 0.8514769077301025,
        "meteor": 0.0819672131147541,
        "sbert_similarity": 0.17844846844673157
      }
    },
    {
      "sample_id": 0,
      "question": "What project did John work on with a game developer by 7 November, 2022?",
      "prediction": "I worked with a game developer on a project and it was great to see my ideas come to life.",
      "reference": "An online board game",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0909090909090909,
        "rouge1_f": 0.08333333333333334,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08333333333333334,
        "bleu1": 0.047619047619047616,
        "bleu2": 0.01543033499620919,
        "bleu3": 0.011280777824436245,
        "bleu4": 0.009134423666564471,
        "bert_precision": 0.8395477533340454,
        "bert_recall": 0.8758229613304138,
        "bert_f1": 0.8573018312454224,
        "meteor": 0.08928571428571429,
        "sbert_similarity": 0.3111250400543213
      }
    },
    {
      "sample_id": 0,
      "question": "What is the name of John's cousin's dog?",
      "prediction": "The content does not provide the name of John's cousin's dog.",
      "reference": "Luna",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8071389198303223,
        "bert_recall": 0.8234508037567139,
        "bert_f1": 0.8152132630348206,
        "meteor": 0.0,
        "sbert_similarity": 0.12200269103050232
      }
    },
    {
      "sample_id": 0,
      "question": "What did John adopt in April 2022?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a pup",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8752520084381104,
        "bert_recall": 0.8777099847793579,
        "bert_f1": 0.8764792680740356,
        "meteor": 0.0,
        "sbert_similarity": 0.06366376578807831
      }
    },
    {
      "sample_id": 0,
      "question": "What is the name of the kitten that was adopted by James?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Ned",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8478286266326904,
        "bert_recall": 0.8958107829093933,
        "bert_f1": 0.8711594343185425,
        "meteor": 0.0,
        "sbert_similarity": 0.09828166663646698
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired John to create the game character in the virtual world?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Appearance of a woman he saw during a walk",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8873249888420105,
        "bert_recall": 0.8567896485328674,
        "bert_f1": 0.8717899918556213,
        "meteor": 0.0,
        "sbert_similarity": 0.11221455782651901
      }
    },
    {
      "sample_id": 0,
      "question": "Which country did John visit in 2021?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Italy",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8448578119277954,
        "bert_recall": 0.8878536820411682,
        "bert_f1": 0.8658222556114197,
        "meteor": 0.0,
        "sbert_similarity": 0.1417396068572998
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of assignment was giving James a hard time at work?",
      "prediction": "Coding assignment",
      "reference": "Coding assignment",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 0.9999998807907104
      }
    },
    {
      "sample_id": 0,
      "question": "What did James and his friends do with the remaining money after helping the dog shelter?",
      "prediction": "Bought groceries and cooked food for the homeless",
      "reference": "Bought groceries and cooked food for the homeless",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9990234375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What was the main goal of the money raised from the political campaign organized by John and his friends in May 2022?",
      "prediction": "Raise money for a dog shelter",
      "reference": "Raise money for a dog shelter",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9976851851851852,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did the system John created help the illegal organization with?",
      "prediction": "tracking inventory, resources, and donations",
      "reference": "tracking inventory, resources, and donations",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did James create for the charitable foundation that helped generate reports for analysis?",
      "prediction": "Not mentioned in the conversation",
      "reference": "computer application on smartphones",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8794436454772949,
        "bert_recall": 0.8623005747795105,
        "bert_f1": 0.8707877397537231,
        "meteor": 0.0,
        "sbert_similarity": 0.051634326577186584
      }
    },
    {
      "sample_id": 0,
      "question": "Who does James support in cricket matches?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Liverpool",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8463980555534363,
        "bert_recall": 0.8887680768966675,
        "bert_f1": 0.8670657873153687,
        "meteor": 0.0,
        "sbert_similarity": 0.18127261102199554
      }
    },
    {
      "sample_id": 0,
      "question": "What is Max good at doing according to John?",
      "prediction": "Not mentioned in the conversation",
      "reference": "catching frisbees in mid-air",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.20000000000000004,
        "rouge2_f": 0.0,
        "rougeL_f": 0.20000000000000004,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.880901038646698,
        "bert_recall": 0.8184996247291565,
        "bert_f1": 0.8485546708106995,
        "meteor": 0.12195121951219512,
        "sbert_similarity": -0.01538090966641903
      }
    },
    {
      "sample_id": 0,
      "question": "Will there be a background check required to volunteer with the organization James volunteered for?",
      "prediction": "Not mentioned in the conversation",
      "reference": "No",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8505663275718689,
        "bert_recall": 0.8942975401878357,
        "bert_f1": 0.8718839287757874,
        "meteor": 0.0,
        "sbert_similarity": 0.19934098422527313
      }
    },
    {
      "sample_id": 0,
      "question": "How did James relax in his free time on 9 July, 2022?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Reading",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8477214574813843,
        "bert_recall": 0.8906177282333374,
        "bert_f1": 0.8686403036117554,
        "meteor": 0.0,
        "sbert_similarity": 0.1694503128528595
      }
    },
    {
      "sample_id": 0,
      "question": "What new hobby did John become interested in on 9 July, 2022?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Extreme sports",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.860535740852356,
        "bert_recall": 0.8390499949455261,
        "bert_f1": 0.8496570587158203,
        "meteor": 0.0,
        "sbert_similarity": 0.07093588262796402
      }
    },
    {
      "sample_id": 0,
      "question": "When did John plan to return from his trip to Toronto and Vancouver?",
      "prediction": "July 20",
      "reference": "July 20",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What made James leave his IT job?",
      "prediction": "to focus on things that align with his values and passions",
      "reference": "to focus on things that align with his values and passions",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9996243425995492,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "Which game tournaments does James plan to organize besides CS:GO?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Fortnite competitions",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8677635192871094,
        "bert_recall": 0.7694649696350098,
        "bert_f1": 0.8156633377075195,
        "meteor": 0.0,
        "sbert_similarity": 0.05143734812736511
      }
    },
    {
      "sample_id": 0,
      "question": "What happened to James's kitten during the recent visit to the clinic?",
      "prediction": "Not mentioned in the conversation",
      "reference": "routine examination and vaccination",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8775582313537598,
        "bert_recall": 0.8384897708892822,
        "bert_f1": 0.857579231262207,
        "meteor": 0.0,
        "sbert_similarity": -0.012585628777742386
      }
    },
    {
      "sample_id": 0,
      "question": "What aspect of \"The Witcher 3\" does John find boring?",
      "prediction": "Not mentioned in the conversation",
      "reference": "shaping the world with choices",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.20000000000000004,
        "rouge1_f": 0.20000000000000004,
        "rouge2_f": 0.0,
        "rougeL_f": 0.20000000000000004,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8635027408599854,
        "bert_recall": 0.8312468528747559,
        "bert_f1": 0.8470678329467773,
        "meteor": 0.10000000000000002,
        "sbert_similarity": 0.03287177532911301
      }
    },
    {
      "sample_id": 0,
      "question": "What is John planning to do after receiving Samantha's phone number?",
      "prediction": "call her",
      "reference": "call her",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9375,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 0,
      "question": "What has James been teaching his siblings?",
      "prediction": "Not mentioned in the conversation",
      "reference": "coding",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.848389744758606,
        "bert_recall": 0.8946167826652527,
        "bert_f1": 0.8708903193473816,
        "meteor": 0.0,
        "sbert_similarity": 0.09024021029472351
      }
    },
    {
      "sample_id": 0,
      "question": "How much does James pay per dance class?",
      "prediction": "$10",
      "reference": "$10",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did James learn to make in the chemistry class besides omelette and meringue?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Dough",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8491778373718262,
        "bert_recall": 0.8949703574180603,
        "bert_f1": 0.8714728951454163,
        "meteor": 0.0,
        "sbert_similarity": 0.08688984811306
      }
    },
    {
      "sample_id": 0,
      "question": "Why did James sign up for a ballet class?",
      "prediction": "Not mentioned in the conversation",
      "reference": "He wanted to learn something new",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8875070810317993,
        "bert_recall": 0.8549302220344543,
        "bert_f1": 0.870914101600647,
        "meteor": 0.0,
        "sbert_similarity": 0.07536205649375916
      }
    },
    {
      "sample_id": 0,
      "question": "What did John prepare for the first time in the cooking class?",
      "prediction": "Omelette",
      "reference": "Omelette",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What is the name of the board game James tried in September 2022?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Dungeons of the Dragon",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8713937401771545,
        "bert_recall": 0.8252997994422913,
        "bert_f1": 0.8477206826210022,
        "meteor": 0.12195121951219512,
        "sbert_similarity": 0.12376389652490616
      }
    },
    {
      "sample_id": 0,
      "question": "Where does John get his ideas from?",
      "prediction": "Not mentioned in the conversation",
      "reference": "books, movies, dreams",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8637099266052246,
        "bert_recall": 0.8087742328643799,
        "bert_f1": 0.8353398442268372,
        "meteor": 0.0,
        "sbert_similarity": 0.12579873204231262
      }
    },
    {
      "sample_id": 0,
      "question": "What did James use to play when he was younger to let off steam?",
      "prediction": "Not mentioned in the conversation",
      "reference": "drums",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8512215614318848,
        "bert_recall": 0.8965955972671509,
        "bert_f1": 0.8733195662498474,
        "meteor": 0.0,
        "sbert_similarity": 0.1397026926279068
      }
    },
    {
      "sample_id": 0,
      "question": "What does James do to stay informed and constantly learn about game design?",
      "prediction": "watch tutorials and keep up with developer forums",
      "reference": "watch tutorials and keep up with developer forums",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9990234375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of gig was James offered at the game dev non-profit organization?",
      "prediction": "Not mentioned in the conversation",
      "reference": "programming mentor for game developers",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8738769888877869,
        "bert_recall": 0.8570197820663452,
        "bert_f1": 0.8653663396835327,
        "meteor": 0.0,
        "sbert_similarity": 0.024469465017318726
      }
    },
    {
      "sample_id": 0,
      "question": "What does James feel about starting the journey as a programming mentor for game developers?",
      "prediction": "excited and inspired",
      "reference": "excited and inspired",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired James to create his painting?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Witcher 3",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8635885715484619,
        "bert_recall": 0.8725632429122925,
        "bert_f1": 0.8680527210235596,
        "meteor": 0.0,
        "sbert_similarity": 0.09098147600889206
      }
    },
    {
      "sample_id": 0,
      "question": "What games were played at the gaming tournament organized by James on 31 October, 2022?",
      "prediction": "Fortnite",
      "reference": "Fortnite, Overwatch, Apex Legends",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.006737946999085467,
        "bleu2": 0.002130725927060655,
        "bleu3": 0.0014741021868805238,
        "bleu4": 0.0011981952414407235,
        "bert_precision": 0.9743298292160034,
        "bert_recall": 0.8765589594841003,
        "bert_f1": 0.9228621125221252,
        "meteor": 0.0,
        "sbert_similarity": 0.6345908045768738
      }
    },
    {
      "sample_id": 0,
      "question": "What was the purpose of the gaming tournament organized by James on 31 October, 2022?",
      "prediction": "To raise money for a children's hospital",
      "reference": "To raise money for a children's hospital",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What decision did John and Samantha make on 31 October, 2022?",
      "prediction": "Not mentioned in the conversation",
      "reference": "To move in together",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8868861198425293,
        "bert_recall": 0.8726798295974731,
        "bert_f1": 0.8797256350517273,
        "meteor": 0.12195121951219512,
        "sbert_similarity": 0.13424353301525116
      }
    },
    {
      "sample_id": 0,
      "question": "Where did John and Samantha decide to live together on 31 October, 2022?",
      "prediction": "Not mentioned in the conversation",
      "reference": "In an apartment not far from McGee's bar",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3076923076923077,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.17973158564688863,
        "bleu2": 0.04493289641172216,
        "bleu3": 0.03199737863082477,
        "bleu4": 0.028709564316081952,
        "bert_precision": 0.8787854909896851,
        "bert_recall": 0.8332972526550293,
        "bert_f1": 0.8554370999336243,
        "meteor": 0.12987012987012989,
        "sbert_similarity": 0.23394805192947388
      }
    },
    {
      "sample_id": 0,
      "question": "Why did John and Samantha choose an apartment near McGee's bar?",
      "prediction": "They love spending time together at the bar",
      "reference": "They love spending time together at the bar",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9990234375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What game is James hooked on playing on 5 November, 2022?",
      "prediction": "Not mentioned in the conversation",
      "reference": "FIFA 23",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.860573947429657,
        "bert_recall": 0.8591500520706177,
        "bert_f1": 0.8598614931106567,
        "meteor": 0.0,
        "sbert_similarity": 0.1617240011692047
      }
    },
    {
      "sample_id": 0,
      "question": "What project did James work on with a game developer by 7 November, 2022?",
      "prediction": "Not mentioned in the conversation",
      "reference": "An online board game",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8739731907844543,
        "bert_recall": 0.8472806215286255,
        "bert_f1": 0.8604199290275574,
        "meteor": 0.0,
        "sbert_similarity": 0.0810113400220871
      }
    },
    {
      "sample_id": 0,
      "question": "What is the name of James's cousin's dog?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Luna",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8471735119819641,
        "bert_recall": 0.8907058835029602,
        "bert_f1": 0.8683944344520569,
        "meteor": 0.0,
        "sbert_similarity": 0.18352878093719482
      }
    }
  ]
}